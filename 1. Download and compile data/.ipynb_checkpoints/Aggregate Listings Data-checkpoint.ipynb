{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Dataset\n",
    "\n",
    "* This Notebook concatenates Airbnb listing files from http://insideairbnb.com/ and creates both wide form and long form aggregate datasets.\n",
    "* The datasets are represent a balanced panel for a given set of unique Airbnb listings. When a listing doesn't appear in a given month it is still asssigned an entry in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_folder = '/united-states_portland'\n",
    "city_abbrev = 'POR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preliminary directory, use of os should make this compatible for any user with access to the repository\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Make sure repository has a 0. Raw data folder!\n",
    "data_dir = cwd2 + '/0. Raw data' + city_folder\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts values into an integer, if it fails return a string.\n",
    "\n",
    "def IntorStr(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting listings.csv.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portland\n",
      "39\n",
      "['united-states_portland_2015-03-01_listings.csv.gz'\n",
      " 'united-states_portland_2015-05-12_listings.csv.gz'\n",
      " 'united-states_portland_2015-09-02_listings.csv.gz'\n",
      " 'united-states_portland_2015-11-02_listings.csv.gz'\n",
      " 'united-states_portland_2015-12-02_listings.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# Collect the listings CSVs\n",
    "\n",
    "numFiles = []\n",
    "fileNames = os.listdir(data_dir)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"_listings.csv.gz\"):\n",
    "        numFiles.append(fileNames)\n",
    "    \n",
    "city = numFiles[0].split(\"_\")[1]\n",
    "print(city)\n",
    "\n",
    "# Count the number of files\n",
    "numFiles = np.sort(numFiles)\n",
    "print(len(numFiles))\n",
    "\n",
    "# Take a look at the first 5 listing files\n",
    "print(numFiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if a file is missing specific columns\n",
    "The loop below accepts a list of data file names and a list of column names and then prints if a file is missing a particular variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N columns: 64\n",
      "['Unnamed: 0', 'id', 'scrape_id', 'last_scraped', 'name', 'picture_url', 'host_id', 'host_name', 'host_since', 'host_picture_url', 'street', 'neighbourhood', 'neighbourhood_cleansed', 'city', 'state', 'zipcode', 'market', 'country', 'latitude', 'longitude', 'is_location_exact', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type', 'square_feet', 'price', 'weekly_price', 'monthly_price', 'guests_included', 'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'host_is_superhost', 'host_about', 'host_neighbourhood', 'host_location', 'host_acceptance_rate', 'host_listings_count', 'host_total_listings_count', 'calculated_host_listings_count', 'requires_license', 'license', 'jurisdiction_names', 'reviews_per_month']\n",
      "---------------------\n",
      "N columns: 69\n",
      "---------------------\n",
      "N columns: 93\n",
      "---------------------\n",
      "N columns: 93\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# First take a look at columns in first four files\n",
    "\n",
    "for my_file in numFiles[0:4]:\n",
    "    data = pd.read_csv(data_dir + '/' + my_file, encoding= 'iso-8859-1')\n",
    "    data_columns = list(data.columns)\n",
    "    print(\"N columns: \" + str(len(data_columns)))\n",
    "\n",
    "    if my_file == numFiles[0]:\n",
    "        print(data_columns)\n",
    "    \n",
    "    print(\"---------------------\")\n",
    "    \n",
    "# Delete first two files because they are missing some variables of interest\n",
    "numFiles = np.delete(numFiles, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_cols(files, variables):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a list of data file names (strings) \n",
    "    and a list of column names (strings) and then prints if \n",
    "    a file is missing a particular variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    for my_file in files:\n",
    "        data = pd.read_csv(data_dir + '/' + my_file, encoding = 'iso-8859-1')\n",
    "        data_columns = list(data.columns)\n",
    "        \n",
    "        for my_column in variables:\n",
    "            if my_column not in data_columns:\n",
    "                print(my_column + \" missing from:\")\n",
    "                print(my_file)\n",
    "        \n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop is relatively slow. It takes about a minute and a half.\n",
    "def find_compatible_columns(most_missing_columns):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a filename (string). The function then \n",
    "    returns a list of the column names that exist in all files\n",
    "    in numFiles based on the column names of the passed filename.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(most_missing_columns.columns)\n",
    "    \n",
    "    for i in numFiles:\n",
    "        data = pd.read_csv(data_dir + '/'  + i, encoding = 'iso-8859-1')\n",
    "        data_columns = set(data.columns)\n",
    "    \n",
    "        if i == numFiles[0]:\n",
    "            compat = data_columns.intersection(a)\n",
    "        \n",
    "        compat = compat.intersection(data_columns)\n",
    "    \n",
    "    compat = list(compat)\n",
    "    \n",
    "    return compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is city-specific. Need to be careful if not using Portland!\n",
    "\n",
    "# This feeds the file with the most missing columns into the above function to create a compatible column set for all the data..\n",
    "most_missing = pd.read_csv(data_dir  + '/united-states_portland_2015-09-02_listings.csv.gz', encoding='iso-8859-1')\n",
    "compatible = find_compatible_columns(most_missing)\n",
    "\n",
    "# Make id the first cell, I think this is more important to the long_form code than deleting id!\n",
    "compatible.remove('id')\n",
    "compatible.insert(0, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code creates and displays a list of important variables not found in the compatible set.\n",
    "\n",
    "airbnb_metrics = ['id', 'last_scraped', 'host_id', 'host_name', \n",
    "                  'host_since', 'host_location', 'host_response_time', 'host_response_rate',\n",
    "                  'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood',\n",
    "                  'neighbourhood_cleansed', 'street', 'zipcode', 'latitude', \n",
    "                  'longitude', 'is_location_exact', 'property_type', 'room_type', \n",
    "                  'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                  'bed_type', 'square_feet', 'price', 'weekly_price',\n",
    "                  'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included',\n",
    "                  'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', \n",
    "                  'calendar_last_scraped', 'has_availability', 'availability_30', 'availability_60', \n",
    "                  'availability_90', 'availability_365', 'number_of_reviews', 'first_review', \n",
    "                  'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                  'requires_license', 'license', 'instant_bookable', 'cancellation_policy',\n",
    "                  'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n",
    "\n",
    "airbnb_metrics_vs_compatible = list(filter(lambda i: i not in compatible, airbnb_metrics))\n",
    "airbnb_metrics_vs_compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to print which datasets are missing the airbnb metrics data.\n",
    "# check_data_cols(numFiles, airbnb_metrics_vs_compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "united-states_portland_2015-09-02_listings.csv.gz\n",
      "united-states_portland_2015-11-02_listings.csv.gz\n",
      "united-states_portland_2015-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2015\n",
      "united-states_portland_2015-09-02_listings.csv.gz\n",
      "united-states_portland_2015-11-02_listings.csv.gz\n",
      "united-states_portland_2015-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2016\n",
      "united-states_portland_2016-01-01_listings.csv.gz\n",
      "united-states_portland_2016-02-03_listings.csv.gz\n",
      "united-states_portland_2016-04-05_listings.csv.gz\n",
      "united-states_portland_2016-05-03_listings.csv.gz\n",
      "united-states_portland_2016-06-03_listings.csv.gz\n",
      "united-states_portland_2016-07-04_listings.csv.gz\n",
      "united-states_portland_2016-08-04_listings.csv.gz\n",
      "united-states_portland_2016-09-04_listings.csv.gz\n",
      "united-states_portland_2016-11-06_listings.csv.gz\n",
      "united-states_portland_2016-12-08_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2017\n",
      "united-states_portland_2017-01-04_listings.csv.gz\n",
      "united-states_portland_2017-02-09_listings.csv.gz\n",
      "united-states_portland_2017-03-05_listings.csv.gz\n",
      "united-states_portland_2017-04-07_listings.csv.gz\n",
      "united-states_portland_2017-05-07_listings.csv.gz\n",
      "united-states_portland_2017-06-05_listings.csv.gz\n",
      "united-states_portland_2017-07-06_listings.csv.gz\n",
      "united-states_portland_2017-08-06_listings.csv.gz\n",
      "united-states_portland_2017-09-12_listings.csv.gz\n",
      "united-states_portland_2017-10-04_listings.csv.gz\n",
      "united-states_portland_2017-11-13_listings.csv.gz\n",
      "united-states_portland_2017-12-09_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2018\n",
      "united-states_portland_2018-01-16_listings.csv.gz\n",
      "united-states_portland_2018-02-08_listings.csv.gz\n",
      "united-states_portland_2018-04-11_listings.csv.gz\n",
      "united-states_portland_2018-05-13_listings.csv.gz\n",
      "united-states_portland_2018-07-10_listings.csv.gz\n",
      "united-states_portland_2018-08-14_listings.csv.gz\n",
      "united-states_portland_2018-09-14_listings.csv.gz\n",
      "united-states_portland_2018-10-09_listings.csv.gz\n",
      "united-states_portland_2018-11-07_listings.csv.gz\n",
      "united-states_portland_2018-12-10_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2019\n",
      "united-states_portland_2019-01-13_listings.csv.gz\n",
      "united-states_portland_2019-02-06_listings.csv.gz\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display files by year\n",
    "\n",
    "years = np.linspace(2015,2019, 6)\n",
    "\n",
    "for year in years:\n",
    "    print(str(int(year)))\n",
    "    for IND in range(len(numFiles)):\n",
    "            if numFiles[IND].split('_')[2][0:4] == str(int(year)):\n",
    "                print(numFiles[IND])\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(concat_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts a starting and ending index of numFiles \n",
    "    as arguments and returns a cocatanated dataframe of the csv data \n",
    "    corresponding to the inputted indexes.\n",
    "    \"\"\"  \n",
    "    yearly_numfiles_bool = []\n",
    "    for filename in numFiles:\n",
    "        if (filename.split('_')[2][0:4] == str(int(concat_year))):\n",
    "            yearly_numfiles_bool.append(True)\n",
    "        else:\n",
    "            yearly_numfiles_bool.append(False)\n",
    "    \n",
    "    sheets_df = []\n",
    "    yearly_numfiles = list(compress(numFiles, yearly_numfiles_bool))\n",
    "    for filename in yearly_numfiles:\n",
    "        df = pd.read_csv(data_dir + '/' + filename, index_col = None, header=0, encoding='iso-8859-1') \n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4986792  3883718  7092722 ... 32129035 32139925 32164834]\n"
     ]
    }
   ],
   "source": [
    "# This code concatenates all datframes for a given year.\n",
    "\n",
    "# Save the concatenated frames as separate files\n",
    "sheet15 = concat_spreadsheets(2015) # 2015\n",
    "sheet16 = concat_spreadsheets(2016) # 2016\n",
    "sheet17 = concat_spreadsheets(2017) # 2017\n",
    "sheet18 = concat_spreadsheets(2018) # 2018\n",
    "sheet19 = concat_spreadsheets(2019) # 2019\n",
    "\n",
    "# Identify all unique listings ids across datasets\n",
    "uniq_all = pd.concat([sheet15.id, sheet16.id, \n",
    "                      sheet17.id, sheet18.id, \n",
    "                      sheet19.id], axis=0, ignore_index=True).unique()\n",
    "\n",
    "print(uniq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wide form of full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_form(UNIQ_IDS, START, END, METRICS): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a list of Airbnb unique ids as well as start\n",
    "    and end indexes for said list. Then, for the selected ID's\n",
    "    it returns a dataframe of the relevant data in a wide format.\n",
    "    \"\"\"\n",
    "    \n",
    "    listing_df = pd.DataFrame(UNIQ_IDS)\n",
    "    listing_df.columns = ['id']\n",
    "    \n",
    "    print(\"Number of unique listings: \" + str(len(listing_df)))\n",
    "    \n",
    "    output_df = listing_df.copy()    \n",
    "    \n",
    "    date_count = START\n",
    "    for i in numFiles[START:END]:\n",
    " \n",
    "        # Read in gzip compressed files\n",
    "        file = gzip.open(os.path.join(data_dir, i), 'rt')  \n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "\n",
    "        bnb_metrics = METRICS\n",
    "        \n",
    "        d={}\n",
    "        for j in bnb_metrics:\n",
    "            d[str(j)+\"_index\"] = headers.index(j)\n",
    "#             print(str(j))\n",
    "#             print(d[str(j)+\"_index\"])\n",
    "\n",
    "        row_values = []\n",
    "\n",
    "        for row in reader:\n",
    "            value_i = []\n",
    "\n",
    "            for j in bnb_metrics:\n",
    "                value_j = IntorStr(row[d[str(j)+\"_index\"]])\n",
    "                value_i.append(value_j)\n",
    "\n",
    "            row_values.append(value_i)\n",
    "        \n",
    "        values_df = pd.DataFrame(row_values) # Create a dataframe for the row_values      \n",
    "        values_df.columns = bnb_metrics # Set column titles\n",
    "        values_df = values_df.drop_duplicates(subset='id', keep='last')\n",
    "\n",
    "        # Merge the values with their respective id and drop duplicates\n",
    "        merged_df = pd.merge(listing_df, values_df, how='outer', on='id')\n",
    "        merged_df = merged_df.drop_duplicates(keep='first')   \n",
    "        merged_df = merged_df.reset_index()\n",
    "\n",
    "        for k in bnb_metrics[1:]: \n",
    "            output_df[k + str(date_count)] = merged_df[k]\n",
    "\n",
    "        output_df['List_month'+str(date_count)] = listing_df['id'].isin(np.array(values_df['id']))*1 # See if the observation is in the month data  \n",
    "\n",
    "        date_count += 1\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique listings: 11307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/l6t8st49137cl77yqnxy4z2m0000gn/T/ipykernel_44064/1205718636.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  output_df[k + str(date_count)] = merged_df[k]\n",
      "/var/folders/nx/l6t8st49137cl77yqnxy4z2m0000gn/T/ipykernel_44064/1205718636.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  output_df['List_month'+str(date_count)] = listing_df['id'].isin(np.array(values_df['id']))*1 # See if the observation is in the month data\n"
     ]
    }
   ],
   "source": [
    "# Wide format for all\n",
    "wideALL = wide_form(uniq_all, 0, len(numFiles), airbnb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save wide dataframe to local directory\n",
    "wideALL.to_csv(city_abbrev + '_Data_wideALL.csv.gz', \n",
    "               compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create long form of full data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_form(df, METRICS):\n",
    "    \"\"\"\n",
    "    Creates a long form data frame when provided with \n",
    "    a wide form dataset.\n",
    "    \"\"\"\n",
    "    bnb_metrics = METRICS\n",
    "    mylist = ['List_month']\n",
    "    mylist.extend(bnb_metrics[1:])\n",
    "    long_df = pd.wide_to_long(df, stubnames=mylist, i='id', j='month')\n",
    "    \n",
    "   \n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full long dataframe creation.\n",
    "longALL = long_form(wideALL, airbnb_metrics)\n",
    "\n",
    "# Resets the long dataframe's index\n",
    "longALL = longALL.reset_index()\n",
    "\n",
    "# Save long dataframe to local directory.\n",
    "longALL.to_csv(city_abbrev + '_Data_longALL.csv.gz', \n",
    "               compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
