{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First version created: October 17, 2019\n",
    "\n",
    "This Notebook concatenates Airbnb listing files from http://insideairbnb.com/ and creates both wide form and long form aggregate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python libraries\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preliminary directory, use of os should make this compatible for any user with access to the repository\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Make sure repository has a 0. Raw data folder!\n",
    "data_dir = cwd2 + '/0. Raw data'\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts values into an integer, if it fails return a string.\n",
    "\n",
    "def IntorStr(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting listings.csv.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "san-francisco\n",
      "58\n",
      "['united-states_san-francisco_2015-09-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2015-11-01_listings.csv.gz'\n",
      " 'united-states_san-francisco_2015-12-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-02-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-04-03_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-05-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-06-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-07-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-08-02_listings.csv.gz'\n",
      " 'united-states_san-francisco_2016-09-02_listings.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# Collect the listings CSVs\n",
    "\n",
    "numFiles = []\n",
    "fileNames = os.listdir(data_dir)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"_listings.csv.gz\"):\n",
    "        numFiles.append(fileNames)\n",
    "    \n",
    "city = numFiles[0].split(\"_\")[1]\n",
    "print(city)\n",
    "\n",
    "# Count the number of files\n",
    "numFiles = np.sort(numFiles)\n",
    "print(len(numFiles))\n",
    "\n",
    "# Take a look at the first 10 listing files\n",
    "print(numFiles[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (a). Check if a file is missing specific columns\n",
    "The loop below accepts a list of data file names and a list of column names and then prints if a file is missing a particular variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N columns: 92\n",
      "['id', 'listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'experiences_offered', 'neighborhood_overview', 'notes', 'transit', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'street', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'city', 'state', 'zipcode', 'market', 'smart_location', 'country_code', 'country', 'latitude', 'longitude', 'is_location_exact', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type', 'amenities', 'square_feet', 'price', 'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'requires_license', 'license', 'jurisdiction_names', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', 'calculated_host_listings_count', 'reviews_per_month']\n",
      "---------------------\n",
      "N columns: 92\n",
      "['id', 'listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'experiences_offered', 'neighborhood_overview', 'notes', 'transit', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'street', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'city', 'state', 'zipcode', 'market', 'smart_location', 'country_code', 'country', 'latitude', 'longitude', 'is_location_exact', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type', 'amenities', 'square_feet', 'price', 'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'requires_license', 'license', 'jurisdiction_names', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', 'calculated_host_listings_count', 'reviews_per_month']\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# First take a look at columns in first two files\n",
    "\n",
    "for my_file in numFiles[0:2]:\n",
    "    data = pd.read_csv(data_dir + '/' + my_file, encoding= 'iso-8859-1')\n",
    "    data_columns = list(data.columns)\n",
    "    print(\"N columns: \" + str(len(data_columns)))\n",
    "    print(data_columns)\n",
    "    \n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_cols(files, variables):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a list of data file names (strings) \n",
    "    and a list of column names (strings) and then prints if \n",
    "    a file is missing a particular variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    for my_file in files:\n",
    "        data = pd.read_csv(data_dir + '/' + my_file, encoding = 'iso-8859-1')\n",
    "        data_columns = list(data.columns)\n",
    "        \n",
    "        for my_column in variables:\n",
    "            if my_column not in data_columns:\n",
    "                print(my_column + \" missing from:\")\n",
    "                print(my_file)\n",
    "        \n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop is relatively slow. It takes about a minute and a half.\n",
    "\n",
    "def find_compatible_columns(most_missing_columns):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a filename (string). The function then \n",
    "    returns a list of the column names that exist in all files\n",
    "    in numFiles based on the column names of the passed filename.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(most_missing_columns.columns)\n",
    "    \n",
    "    for i in numFiles:\n",
    "        data = pd.read_csv(data_dir + '/'  + i, encoding = 'iso-8859-1')\n",
    "        data_columns = set(data.columns)\n",
    "    \n",
    "        if i == numFiles[0]:\n",
    "            compat = data_columns.intersection(a)\n",
    "        \n",
    "        compat = compat.intersection(data_columns)\n",
    "    \n",
    "    compat = list(compat)\n",
    "    \n",
    "    return compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lvk99/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (43) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/Users/lvk99/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (61,62) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# This feeds the file with the most missing columns into the above function to create a compatible column set for all the data..\n",
    "most_missing = pd.read_csv(data_dir + '/'  + 'united-states_san-francisco_2015-09-02_listings.csv.gz', encoding='iso-8859-1')\n",
    "compatible = find_compatible_columns(most_missing)\n",
    "\n",
    "# Make id the first cell, I think this is more important to the long_form code than deleting id!\n",
    "compatible.remove('id')\n",
    "compatible.insert(0, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code creates and displays a list of important variables not found in the compatible set.\n",
    "\n",
    "airbnb_metrics = ['id', 'last_scraped', 'host_id', 'host_name', \n",
    "                  'host_since', 'host_location', 'host_response_time', 'host_response_rate',\n",
    "                  'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood',\n",
    "                  'neighbourhood_cleansed', 'street', 'zipcode', 'latitude', \n",
    "                  'longitude', 'is_location_exact', 'property_type', 'room_type', \n",
    "                  'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                  'bed_type', 'square_feet', 'price', 'weekly_price',\n",
    "                  'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included',\n",
    "                  'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', \n",
    "                  'calendar_last_scraped', 'has_availability', 'availability_30', 'availability_60', \n",
    "                  'availability_90', 'availability_365', 'number_of_reviews', 'first_review', \n",
    "                  'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                  'requires_license', 'license', 'instant_bookable', 'cancellation_policy',\n",
    "                  'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n",
    "\n",
    "airbnb_metrics_vs_compatible = list(filter(lambda i: i not in compatible, airbnb_metrics))\n",
    "airbnb_metrics_vs_compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to print which datasets are missing the airbnb metrics data.\n",
    "# check_data_cols(numFiles, airbnb_metrics_vs_compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "united-states_san-francisco_2015-09-02_listings.csv.gz\n",
      "united-states_san-francisco_2015-11-01_listings.csv.gz\n",
      "united-states_san-francisco_2015-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2016\n",
      "united-states_san-francisco_2016-02-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-04-03_listings.csv.gz\n",
      "united-states_san-francisco_2016-05-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-06-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-07-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-08-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-09-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-10-01_listings.csv.gz\n",
      "united-states_san-francisco_2016-11-02_listings.csv.gz\n",
      "united-states_san-francisco_2016-12-03_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2017\n",
      "united-states_san-francisco_2017-01-01_listings.csv.gz\n",
      "united-states_san-francisco_2017-02-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-03-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-04-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-05-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-06-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-07-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-08-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-09-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-10-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-11-01_listings.csv.gz\n",
      "united-states_san-francisco_2017-11-08_listings.csv.gz\n",
      "united-states_san-francisco_2017-12-02_listings.csv.gz\n",
      "united-states_san-francisco_2017-12-07_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2018\n",
      "united-states_san-francisco_2018-01-10_listings.csv.gz\n",
      "united-states_san-francisco_2018-01-17_listings.csv.gz\n",
      "united-states_san-francisco_2018-02-02_listings.csv.gz\n",
      "united-states_san-francisco_2018-03-04_listings.csv.gz\n",
      "united-states_san-francisco_2018-04-06_listings.csv.gz\n",
      "united-states_san-francisco_2018-05-09_listings.csv.gz\n",
      "united-states_san-francisco_2018-07-05_listings.csv.gz\n",
      "united-states_san-francisco_2018-08-06_listings.csv.gz\n",
      "united-states_san-francisco_2018-09-08_listings.csv.gz\n",
      "united-states_san-francisco_2018-10-03_listings.csv.gz\n",
      "united-states_san-francisco_2018-11-03_listings.csv.gz\n",
      "united-states_san-francisco_2018-12-06_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2019\n",
      "united-states_san-francisco_2019-01-09_listings.csv.gz\n",
      "united-states_san-francisco_2019-02-01_listings.csv.gz\n",
      "united-states_san-francisco_2019-03-06_listings.csv.gz\n",
      "united-states_san-francisco_2019-04-03_listings.csv.gz\n",
      "united-states_san-francisco_2019-05-03_listings.csv.gz\n",
      "united-states_san-francisco_2019-06-02_listings.csv.gz\n",
      "united-states_san-francisco_2019-07-08_listings.csv.gz\n",
      "united-states_san-francisco_2019-08-06_listings.csv.gz\n",
      "united-states_san-francisco_2019-09-12_listings.csv.gz\n",
      "united-states_san-francisco_2019-10-14_listings.csv.gz\n",
      "united-states_san-francisco_2019-11-01_listings.csv.gz\n",
      "united-states_san-francisco_2019-12-04_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2020\n",
      "united-states_san-francisco_2020-01-02_listings.csv.gz\n",
      "united-states_san-francisco_2020-01-04_listings.csv.gz\n",
      "united-states_san-francisco_2020-02-12_listings.csv.gz\n",
      "united-states_san-francisco_2020-03-13_listings.csv.gz\n",
      "united-states_san-francisco_2020-04-07_listings.csv.gz\n",
      "united-states_san-francisco_2020-05-06_listings.csv.gz\n",
      "united-states_san-francisco_2020-06-08_listings.csv.gz\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display files by year\n",
    "\n",
    "years = np.linspace(2015,2020, 6)\n",
    "\n",
    "for year in years:\n",
    "    print(str(int(year)))\n",
    "    for IND in range(len(numFiles)):\n",
    "            if numFiles[IND].split('_')[2][0:4] == str(int(year)):\n",
    "                print(numFiles[IND])\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(concat_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts a starting and ending index of numFiles \n",
    "    as arguments and returns a cocatanated dataframe of the csv data \n",
    "    corresponding to the inputted indexes.\n",
    "    \"\"\"  \n",
    "    yearly_numfiles_bool = []\n",
    "    for filename in numFiles:\n",
    "        if (filename.split('_')[2][0:4] == str(int(concat_year))):\n",
    "            yearly_numfiles_bool.append(True)\n",
    "        else:\n",
    "            yearly_numfiles_bool.append(False)\n",
    "    \n",
    "    sheets_df = []\n",
    "    yearly_numfiles = list(compress(numFiles, yearly_numfiles_bool))\n",
    "    for filename in yearly_numfiles:\n",
    "        df = pd.read_csv(data_dir + '/' + filename, index_col = None, header=0, compression='gzip', encoding='iso-8859-1') \n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code concatenates all spreadsheets for a given year.\n",
    "\n",
    "# Save the concatenated sheets as separate files\n",
    "sheet15 = concat_spreadsheets(2015) # 2015\n",
    "sheet16 = concat_spreadsheets(2016) # 2016\n",
    "sheet17 = concat_spreadsheets(2017) # 2017\n",
    "sheet18 = concat_spreadsheets(2018) # 2018\n",
    "sheet19 = concat_spreadsheets(2019) # 2019\n",
    "\n",
    "# Identify all unique listings ids across datasets\n",
    "uniq_all = pd.concat([sheet15.id, sheet16.id, sheet17.id, sheet18.id, sheet19.id], axis=0, ignore_index=True).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6938818,  2316478,  3168359, ..., 40547706, 40560328, 40569280])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create wide form of full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = data_dir\n",
    "def wide_form(UNIQ_IDS, START, END, METRICS): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a list of Airbnb unique ids as well as start\n",
    "    and end indexes for said list. Then, for the selected ID's\n",
    "    it returns a dataframe of the relevant data in a wide format.\n",
    "    \"\"\"\n",
    "    \n",
    "    listing_df = pd.DataFrame(UNIQ_IDS)\n",
    "    listing_df.columns = ['id']\n",
    "    \n",
    "    print(\"Number of unique listings: \" + str(len(listing_df)))\n",
    "    \n",
    "    output_df = listing_df.copy()    \n",
    "    \n",
    "    date_count = START\n",
    "    for i in numFiles[START:END]:\n",
    " \n",
    "        # Read in gzip compressed files\n",
    "        file = gzip.open(os.path.join(raw_data_path, i), 'rt') \n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "\n",
    "        bnb_metrics = METRICS\n",
    "        \n",
    "        d={}\n",
    "        for j in bnb_metrics:\n",
    "            d[str(j)+\"_index\"] = headers.index(j)\n",
    "#             print(str(j))\n",
    "#             print(d[str(j)+\"_index\"])\n",
    "\n",
    "        row_values = []\n",
    "\n",
    "        for row in reader:\n",
    "            value_i = []\n",
    "\n",
    "            for j in bnb_metrics:\n",
    "                value_j = IntorStr(row[d[str(j)+\"_index\"]])\n",
    "                value_i.append(value_j)\n",
    "\n",
    "            row_values.append(value_i)\n",
    "        \n",
    "        values_df = pd.DataFrame(row_values) # Create a dataframe for the row_values      \n",
    "        values_df.columns = bnb_metrics # Set column titles\n",
    "        values_df = values_df.drop_duplicates(subset='id', keep='last')\n",
    "\n",
    "        # Merge the values with their respective id and drop duplicates\n",
    "        merged_df = pd.merge(listing_df, values_df, how='outer', on='id')\n",
    "        merged_df = merged_df.drop_duplicates(keep='first')   \n",
    "        merged_df = merged_df.reset_index()\n",
    "\n",
    "        for k in bnb_metrics[1:]: # I don't want this loop to include 'id', hence [1:] range is used\n",
    "            output_df[k + str(date_count)] = merged_df[k]\n",
    "\n",
    "        output_df['List_month'+str(date_count)] = listing_df['id'].isin(np.array(values_df['id']))*1 # See if the observation is in the month data  \n",
    "\n",
    "        date_count += 1\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique listings: 30879\n"
     ]
    }
   ],
   "source": [
    "# Wide format for all\n",
    "wideALL = wide_form(uniq_all, 0, len(numFiles), airbnb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save wide dataframe to local directory\n",
    "\n",
    "wideALL.to_csv('Data_wideALL.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create long form of full data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_form(df, METRICS):\n",
    "    \"\"\"\n",
    "    Creates a long form data frame when provided with \n",
    "    a wide form dataset.\n",
    "    \"\"\"\n",
    "    bnb_metrics = METRICS\n",
    "    mylist = ['List_month']\n",
    "    mylist.extend(bnb_metrics[1:])\n",
    "    long_df = pd.wide_to_long(df, stubnames=mylist, i='id', j='month')\n",
    "    \n",
    "   \n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full long dataframe creation.\n",
    "longALL = long_form(wideALL, airbnb_metrics)\n",
    "# Resets the long dataframe's index\n",
    "longALL = longALL.reset_index()\n",
    "\n",
    "# Save long dataframe to local directory.\n",
    "longALL.to_csv('Data_longALL_v1.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
