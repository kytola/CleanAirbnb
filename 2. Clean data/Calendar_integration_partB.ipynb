{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar integration, partB\n",
    "\n",
    "This Jupyter Notebook brings together all file types on Inside Airbnb (listings.csv.gz, calendar.csv.gz, reviews.csv.gz) to form a dataset on nightly property pricing, availability, and booking transactions. Using the listings files, this dataset also includes property characteristics such as the number of bedrooms, the room type, its review score and host characteristics.\n",
    "\n",
    "The code below relies on previously run Notebooks “1st_stage_panel_data_cleaning.ipynb” and “Calendar_integration_partA.ipynb”. In particular, this notebook will load in:  \n",
    "* [CITY]_1stStageClean.csv.gz file created in the “1st_stage_panel_data_cleaning” notebook\n",
    "* [CITY]_rejoined_booked_df.csv  created in the “Calendar_integration_partA” notebook\n",
    "\n",
    "Review files must also be saved in the Raw Data path in order for the code below to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "# Enable garbage collection module for memory purposes\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = 'united-states_portland/'\n",
    "city_abbrev = 'POR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/' + city_folder\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review assumption\n",
    "\n",
    "I make an assumption that a property must have been 'active' ten days before it recieved a review. This assumption can be adjusted by a researcher in the follow cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many days before a review a property is assumed to be active. \n",
    "N_days_before = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-03' '2015-05' '2015-09' '2015-11' '2015-12' '2016-01' '2016-02'\n",
      " '2016-04' '2016-05' '2016-06' '2016-07' '2016-08' '2016-09' '2016-11'\n",
      " '2016-12' '2017-01' '2017-02' '2017-03' '2017-04' '2017-05' '2017-06'\n",
      " '2017-07' '2017-08' '2017-09' '2017-10' '2017-11' '2017-12' '2018-01'\n",
      " '2018-02' '2018-04' '2018-05' '2018-07' '2018-08' '2018-09' '2018-10'\n",
      " '2018-11' '2018-12' '2019-01' '2019-02']\n"
     ]
    }
   ],
   "source": [
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)      \n",
    "\n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "file_dates = np.sort(np.array(file_dates).astype('datetime64[M]'))\n",
    "\n",
    "print(file_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Upload listings dataframe/cross sectional file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "\n",
    "listings_df = pd.read_csv(city_abbrev + '_1stStageClean.csv.gz', compression = 'gzip', \n",
    "                          low_memory=False, parse_dates=dateCols)\n",
    "\n",
    "clean_months = listings_df['scrape_batch'].astype('datetime64[M]').unique().astype('datetime64[M]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add free parking and pool amenities\n",
    "parking_search = ['Free parking on premises']\n",
    "listings_df.loc[:, 'free_park'] = (listings_df['amenities'].str.contains('|'.join(parking_search), na=False)*1).values\n",
    "\n",
    "pool_search = ['pool', 'Pool']\n",
    "listings_df.loc[:, 'pool'] = (listings_df['amenities'].str.contains('|'.join(pool_search), na=False)*1).values\n",
    "\n",
    "os.chdir(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booked</th>\n",
       "      <th>id</th>\n",
       "      <th>res_date</th>\n",
       "      <th>never_avail</th>\n",
       "      <th>price</th>\n",
       "      <th>last_date</th>\n",
       "      <th>all_prices</th>\n",
       "      <th>final_prices</th>\n",
       "      <th>seen_avail</th>\n",
       "      <th>week_yr</th>\n",
       "      <th>mo_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>9356</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>45890</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>47326</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>222209</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>47674</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booked      id   res_date  never_avail  price last_date  all_prices  \\\n",
       "0   False    9356 2015-09-02        False    NaN       NaN        70.0   \n",
       "1   False   45890 2015-09-02         True    NaN       NaN         NaN   \n",
       "2   False   47326 2015-09-02         True    NaN       NaN         NaN   \n",
       "3   False  222209 2015-09-02        False    NaN       NaN       118.0   \n",
       "4   False   47674 2015-09-02         True    NaN       NaN         NaN   \n",
       "\n",
       "   final_prices  seen_avail  week_yr    mo_yr  \n",
       "0          70.0           1  2015-35  2015-09  \n",
       "1           NaN           0  2015-35  2015-09  \n",
       "2           NaN           0  2015-35  2015-09  \n",
       "3         118.0           1  2015-35  2015-09  \n",
       "4           NaN           0  2015-35  2015-09  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the booked_df that is created by previous partA Jupyter Notebook\n",
    "os.chdir(data_path)\n",
    "booked_df = pd.read_csv(city_abbrev + '_rejoined_booked_df.csv.gz', low_memory=False, compression='gzip')\n",
    "booked_df['res_date'] = booked_df['res_date'].astype('datetime64[D]')\n",
    "booked_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Incorporating review files\n",
    "\n",
    "If a property receives a review then it is assumed that it was available for a certain number of days prior to that review.\n",
    "\n",
    "The commented out code in this section creates the review dataframe as long as you have all of the review scrapes in the \"csv_path\" folder. After it has been created, all that needs to be done is to load it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Collect review csv names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect reviews csvs names\n",
    "\n",
    "os.chdir(csv_path)\n",
    "revFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"reviews.csv.gz\"):\n",
    "        revFiles.append(fileNames)\n",
    "        \n",
    "revFiles = np.sort(revFiles)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create a function to concatenate review dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(possible_files, eligible_files):\n",
    "    sheets_df = []\n",
    "\n",
    "    for filename in revFiles[np.isin(possible_files, possible_files)]:\n",
    "        df = pd.read_csv(filename, index_col = None, header=0)\n",
    "        sheets_df.append(df)\n",
    "\n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Run the function and save a compressed dataframe with all unique reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7090339 497526\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE, THIS CREATES THE AGGREGATE REVIEW FILE\n",
    "\n",
    "# ============================================================\n",
    "# CREATE REVS FILE\n",
    "\n",
    "collected_revs = concat_spreadsheets(file_dates, clean_months)\n",
    "uniq_revs = collected_revs.drop_duplicates()\n",
    "print(len(collected_revs), len(uniq_revs))\n",
    "uniq_revs.to_csv(city_abbrev + '_Revs.csv.gz', compression='gzip', index=False) \n",
    "\n",
    "# ============================================================\n",
    "# LOAD IN ALREADY CREATED REVS FILE\n",
    "\n",
    "# This cell imports the compressed review datafame created above. \n",
    "\n",
    "#os.chdir(csv_path)\n",
    "\n",
    "#uniq_revs = pd.read_csv(city_abbrev + '_Revs.csv.gz', compression='gzip')\n",
    "\n",
    "#os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the listing IDs that are not in the calendar files\n",
    "unique_ids = booked_df.id.unique()\n",
    "uniq_revs = uniq_revs[uniq_revs.listing_id.isin(unique_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lauri/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93710</th>\n",
       "      <td>6541045</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93937</th>\n",
       "      <td>6290791</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94091</th>\n",
       "      <td>2660141</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95435</th>\n",
       "      <td>6361965</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97143</th>\n",
       "      <td>6985337</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       listing_id       date\n",
       "93710     6541045 2015-09-02\n",
       "93937     6290791 2015-09-02\n",
       "94091     2660141 2015-09-02\n",
       "95435     6361965 2015-09-02\n",
       "97143     6985337 2015-09-02"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_date_min, res_date_max = booked_df['res_date'].min(), booked_df['res_date'].max() \n",
    "revs_df = uniq_revs[['listing_id', 'date']]\n",
    "\n",
    "revs_df.loc[:, 'date'] = revs_df['date'].astype('datetime64[D]')\n",
    "mask = (revs_df['date'] >= res_date_min) & (revs_df['date'] <= res_date_max)\n",
    "revs_df = revs_df[mask]\n",
    "revs_short = revs_df \n",
    "\n",
    "# Dataframe of res_dates\n",
    "reviews_file = revs_short\n",
    "N_reviews = len(reviews_file)\n",
    "\n",
    "reviews_file.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Active property dataframe creation\n",
    "\n",
    "Much like the review code, the commented out block below creates a pandas dataframe that tracks the dates during which a property is active.\n",
    "\n",
    "Once the dataframe is created, it can just be loaded in, meaning you can comment out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397172\n"
     ]
    }
   ],
   "source": [
    "# Here we identify the unique dates from the reviews_file and identify the \n",
    "# index for every row of the reviews_file that maps to the unique dates file. \n",
    "# This allows one to avoid duplicate calculations for identical dates.\n",
    "\n",
    "my_dates = reviews_file['date'].astype('datetime64[D]')\n",
    "\n",
    "unq_dates = pd.DataFrame([my_dates.unique()]).T\n",
    "unq_dates.columns = ['unq_dates']\n",
    "unq_dates = unq_dates.sort_values(by='unq_dates').reset_index(drop=True)\n",
    "\n",
    "# Identify the valid index in the unq_arr for the non-unique list of dates\n",
    "\n",
    "dates_arr = my_dates.values\n",
    "unq_arr = np.concatenate(unq_dates.values)\n",
    "\n",
    "sorter = np.argsort(unq_arr)\n",
    "my_indices = sorter[np.searchsorted(unq_arr, dates_arr, sorter=sorter)]\n",
    "\n",
    "my_dates = pd.DataFrame(my_dates.values, columns = ['dates'])\n",
    "my_dates.loc[:, \"unq_ind\"] = my_indices\n",
    "unq_dates.loc[np.r_[my_dates['unq_ind']], :]\n",
    "\n",
    "# Create all of the date ranges that we want to work with.\n",
    "\n",
    "unq_dates.loc[:,\"start\"] = unq_dates['unq_dates'] - datetime.timedelta(days=N_days_before)\n",
    "\n",
    "days_before = []\n",
    "\n",
    "for i in range(len(unq_dates)):\n",
    "      days_before.append(pd.date_range(unq_dates['start'][i], unq_dates['unq_dates'][i], closed='left'))\n",
    "    \n",
    "# # Here we create a dataframe that has all of the listing data\n",
    "\n",
    "active_dates = np.concatenate(np.array(days_before)[my_dates['unq_ind'].values].astype('datetime64[D]'))\n",
    "my_date_ids = np.reshape(pd.concat([reviews_file['listing_id']]*N_days_before, axis=1).values, (N_days_before*N_reviews,1))\n",
    "my_date_ids = np.concatenate(my_date_ids)\n",
    "\n",
    "active_df = pd.DataFrame([my_date_ids, active_dates]).T\n",
    "active_df.columns = ['id', 'rev_active_date']\n",
    "\n",
    "active_day_int = active_df['rev_active_date'].view(np.int64)//(10**3)\n",
    "active_df.loc[:, \"dy_key\"] = active_day_int + active_df['id']\n",
    "del active_day_int\n",
    "\n",
    "# active_df.loc[:, \"key\"] = active_df['id'].astype(str) + \":\" + active_df['date'].astype(str)\n",
    "\n",
    "print(N_reviews)\n",
    "\n",
    "active_df.to_csv('active_dates'+ str(N_days_before) + \".csv.gz\", compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE. my_active_file is created by the code that is commented out below! \n",
    "# [Note: Need to be in the 0. Raw data directory for this to work.]\n",
    "\n",
    "#my_active_file = \"active_dates\" + str(N_days_before) + \".csv.gz\"\n",
    "\n",
    "#active_df = pd.read_csv(my_active_file, compression='gzip', parse_dates=['date'])\n",
    "#active_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Create merge key in the booked_df\n",
    "* Need keys for date and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df_day_int = booked_df['res_date'].astype('datetime64[ns]').values.astype(np.int64)//(10**3)\n",
    "booked_df.loc[:, 'dy_key'] = booked_df_day_int + booked_df['id']\n",
    "\n",
    "del booked_df_day_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_mo_yr_int64 = booked_df['mo_yr'].astype('datetime64').values.view(np.int64)//(10**3)\n",
    "booked_df.loc[:, \"mo_key\"] = booked_mo_yr_int64 + booked_df['id']\n",
    "del booked_mo_yr_int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Create merge key in the listings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df['batch_YRMO'] = listings_df['batch_YRMO'].astype('datetime64').dt.to_period('M')\n",
    "\n",
    "listings_mask = ((listings_df['batch_YRMO'] >= booked_df['mo_yr'].min()) & \n",
    "                 (listings_df['batch_YRMO'] <= booked_df['mo_yr'].max()))\n",
    "\n",
    "listings_df = listings_df[listings_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df_mo_yr_int64 = listings_df['batch_YRMO'].dt.to_timestamp().values.astype(np.int64)//(10**3)\n",
    "listings_df.loc[:, \"mo_key\"] = listings_df_mo_yr_int64 + listings_df['id']\n",
    "\n",
    "del listings_df_mo_yr_int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Merge calendar data and listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename prices to make it clearer\n",
    "booked_df = booked_df.rename(columns={'final_prices':\"calendar_price\"})\n",
    "listings_df = listings_df.rename(columns={'price':\"headline_price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge time-varying variables from the panel to the booked_df dataframe. Prices here are \"headline prices\".\n",
    "list_month_merge = booked_df.merge(listings_df[['mo_key', 'List_month', 'Listlead1', \n",
    "                                                'Listlag1', 'headline_price', 'cleaning_fee', 'host_listings_count', 'cum_sum',\n",
    "                                                 'bedrooms', 'room_type', 'neighbourhood', 'zipcode', # I wanted to have this last row separte \n",
    "                                               'free_park', 'pool', 'host_since', 'review_scores_rating', 'number_of_reviews']], # These two are new here\n",
    "                                   left_on='mo_key', right_on='mo_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some memory\n",
    "del booked_df\n",
    "del listings_df\n",
    "del revs_df\n",
    "del listings_mask\n",
    "del revs_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13304620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booked</th>\n",
       "      <th>id</th>\n",
       "      <th>res_date</th>\n",
       "      <th>never_avail</th>\n",
       "      <th>price</th>\n",
       "      <th>last_date</th>\n",
       "      <th>all_prices</th>\n",
       "      <th>calendar_price</th>\n",
       "      <th>seen_avail</th>\n",
       "      <th>week_yr</th>\n",
       "      <th>...</th>\n",
       "      <th>cum_sum</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>room_type</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>free_park</th>\n",
       "      <th>pool</th>\n",
       "      <th>host_since</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>number_of_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>9356</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>97211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>97.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>45890</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Roseway</td>\n",
       "      <td>97213.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-07-24</td>\n",
       "      <td>97.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>47326</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>Overlook</td>\n",
       "      <td>97217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-08-26</td>\n",
       "      <td>89.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>222209</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>Forest Park</td>\n",
       "      <td>97229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>92.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>47674</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Buckman</td>\n",
       "      <td>97214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   booked      id   res_date  never_avail  price last_date  all_prices  \\\n",
       "0   False    9356 2015-09-02        False    NaN       NaN        70.0   \n",
       "1   False   45890 2015-09-02         True    NaN       NaN         NaN   \n",
       "2   False   47326 2015-09-02         True    NaN       NaN         NaN   \n",
       "3   False  222209 2015-09-02        False    NaN       NaN       118.0   \n",
       "4   False   47674 2015-09-02         True    NaN       NaN         NaN   \n",
       "\n",
       "   calendar_price  seen_avail  week_yr  ... cum_sum  bedrooms  \\\n",
       "0            70.0           1  2015-35  ...     1.0       2.0   \n",
       "1             NaN           0  2015-35  ...     2.0       1.0   \n",
       "2             NaN           0  2015-35  ...     2.0       1.0   \n",
       "3           118.0           1  2015-35  ...     3.0       1.0   \n",
       "4             NaN           0  2015-35  ...     1.0       1.0   \n",
       "\n",
       "         room_type  neighbourhood  zipcode  free_park  pool  host_since  \\\n",
       "0  Entire home/apt       Woodlawn  97211.0        0.0   0.0  2009-08-06   \n",
       "1  Entire home/apt        Roseway  97213.0        0.0   0.0  2010-07-24   \n",
       "2     Private room       Overlook  97217.0        0.0   0.0  2010-08-26   \n",
       "3     Private room    Forest Park  97229.0        0.0   0.0  2011-07-29   \n",
       "4  Entire home/apt        Buckman  97214.0        0.0   0.0  2010-05-07   \n",
       "\n",
       "   review_scores_rating  number_of_reviews  \n",
       "0                  97.0               57.0  \n",
       "1                  97.0               92.0  \n",
       "2                  89.0              221.0  \n",
       "3                  92.0               20.0  \n",
       "4                   NaN                NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_month_merge))\n",
    "list_month_merge.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Merge review data with previous merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mins to run\n",
      "35.1991144657135\n"
     ]
    }
   ],
   "source": [
    "N_chunk_ids = 25 # This chunk size determined through trial and error\n",
    "N_total_chunks = int(np.ceil(active_df['id'].nunique()/N_chunk_ids))\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "full_merge = pd.DataFrame()\n",
    "\n",
    "my_timer = time.time() # Time it\n",
    "\n",
    "for i in range(N_total_chunks):\n",
    "    chunk_ids = active_df['id'].unique()[N_chunk_ids*i:N_chunk_ids*(i+1)]\n",
    "    \n",
    "    active_df_small = active_df[active_df['id'].isin(chunk_ids)]\n",
    "    \n",
    "    list_month_merge_small = list_month_merge[list_month_merge['id'].isin(chunk_ids)]\n",
    "     \n",
    "    partial_merge = list_month_merge_small.merge(active_df_small[['rev_active_date', 'dy_key']], \n",
    "                                 left_on='dy_key', right_on='dy_key', how='left')\n",
    "    \n",
    "    partial_merge = partial_merge.drop(columns=['dy_key', 'mo_key'])\n",
    "    \n",
    "    full_merge = full_merge.append(partial_merge)\n",
    "    \n",
    "ttr = time.time() - my_timer\n",
    "\n",
    "print(\"Mins to run\")\n",
    "print(ttr/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Clean up and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge = full_merge.drop_duplicates()\n",
    "full_merge = full_merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025758, 5712663, 5668738)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare price measures that are null\n",
    "(~full_merge['price'].isna()).sum(), (~full_merge['all_prices'].isna()).sum(), (~full_merge['calendar_price'].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['booked', 'id', 'res_date', 'never_avail', 'last_date',\n",
      "       'calendar_price', 'seen_avail', 'week_yr', 'mo_yr', 'List_month',\n",
      "       'Listlead1', 'Listlag1', 'headline_price', 'cleaning_fee',\n",
      "       'host_listings_count', 'cum_sum', 'bedrooms', 'room_type',\n",
      "       'neighbourhood', 'zipcode', 'free_park', 'pool', 'host_since',\n",
      "       'review_scores_rating', 'number_of_reviews', 'rev_active_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop 'prices' and 'all_prices', which were just intermediary price measures.\n",
    "\n",
    "full_merge = full_merge.drop(columns=['price', 'all_prices'])\n",
    "print(full_merge.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge['rev_active'] = 1 - full_merge['rev_active_date'].isna()*1\n",
    "full_merge['composite_active'] = full_merge[['seen_avail', 'rev_active']].values.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13271398, 28)\n"
     ]
    }
   ],
   "source": [
    "full_merge_tosave = full_merge.copy()\n",
    "full_merge_tosave = full_merge_tosave.drop_duplicates()\n",
    "\n",
    "print(full_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "\n",
    "full_merge_tosave.to_csv(city_abbrev + \"_cal_rev_list_FULLMERGE_\" + \n",
    "                         str(N_days_before) + \"_days.csv.gz\", compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
