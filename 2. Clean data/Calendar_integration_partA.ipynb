{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar integration, part A\n",
    "\n",
    "This Jupyter Notebook allows a researcher to load in and organize the compressed “calendar.csv.gz” files available from Inside Airbnb. These calendar files contain information about a given listing’s availability and pricing for the next 365 days starting from the date of the scrape. The daily pricing and availability information provides greater detail on how hosts adjust prices to account for weekends, holidays and other special events throughout the year. The headline prices contained in the previous “1st_stage_panel_data_cleaning” notebook greatly understate the variation in prices that Airbnb consumers actually face.\n",
    "\n",
    "The current Notebook also calculates the number of bookings for properties by leveraging changes in availability across scrape dates and attempts to find the price associated with these bookings by using the last seen price before the property became unavailable. This approach shares similarities to the approach in Williams (2017), *Dynamic Airline Pricing and Seat Availability*, in which the author identifies airline bookings by observing changes in the seat availability on flights. Since scrapes on Inside Airbnb occur on the monthly basis, the measures for bookings will be relatively crude. I leverage review data to further augment changes in availability in the second part of the calendar integration work (“Calendar_integration_partB”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import time\n",
    "from pandas.core.common import SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = 'united-states_portland/'\n",
    "city_abbrev = 'POR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/' + city_folder\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminary processing\n",
    "\n",
    "Select relevant calendar files, load in previously created dataframe on monthly listings, and set some criteria by which to clean the calendar files.\n",
    "\n",
    "It is also necessary to adjust for the fact that some monthly scrapes took multiple days. The reason for this adjustment is that that there will be different amounts of dates in scrapes that took longer to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop aggregates all calendar file names\n",
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)\n",
    "        \n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "init_dates = file_dates\n",
    "init_dates = np.sort(init_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 1st stage cleaned dataframe\n",
    "os.chdir(data_path)\n",
    "\n",
    "dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "listings_df = pd.read_csv(city_abbrev + '_1stStageClean.csv.gz', compression = 'gzip', low_memory=False, parse_dates=dateCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the calendar dates that exist in the cleaned 1st stage file\n",
    "\n",
    "# Find month-yr for calendar files and for the clean \n",
    "# listings to create a flag.\n",
    "\n",
    "calfile_MOYR = np.sort(init_dates).astype('datetime64[M]')\n",
    "cleanlistings_MOYR = np.sort(listings_df['batch_YRMO'].unique()).astype('datetime64[M]')\n",
    "\n",
    "# Identify which dates are contained in the clean listings data\n",
    "included_in_clean_flag = np.isin(calfile_MOYR,\n",
    "        cleanlistings_MOYR)\n",
    "\n",
    "init_dates = init_dates[included_in_clean_flag].astype('datetime64[D]').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_extract_cal_list(date, cal_start, cal_end, update_cutoff=90, avail_cutoff=0):\n",
    "\n",
    "    \"\"\"\n",
    "    This function removes listings that appear dormant \n",
    "    on Airbnb from the calendar files that we will be using.\n",
    "    \n",
    "    Dormancy is determined by when the calendar was last updated, whether a property is instant bookable,\n",
    "    and based on future availability of the property.\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.simplefilter(\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "    # Managing listings dataframe \n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    \n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv.gz\", \n",
    "                            low_memory = False, parse_dates = dateCols)\n",
    "    \n",
    "    small = list_file['calendar_updated'].str.split(' ', 3, expand=True)\n",
    "    small.columns = ['count', 'measure', 'length']\n",
    "    small = small[['count', 'measure']]\n",
    "    small = small.replace([\"days\", 'week', 'weeks', 'months', 'today', 'never','yesterday'], \n",
    "                          [1, 7, 7, 30, 0, 999, 1])\n",
    "    \n",
    "    small['count'] = small['count'].replace('a', 1)\n",
    "    small = small.fillna(1)\n",
    "\n",
    "    list_file.loc[:, 'update_numeric'] = small['count'].astype(float)*small['measure'].astype(float)\n",
    "\n",
    "    checks = list_file[['update_numeric', 'instant_bookable', 'last_scraped', 'last_review', 'availability_365']]\n",
    "    checks.loc[:, 'DSR'] = (checks['last_scraped'] - checks['last_review']).dt.days\n",
    "    checks = checks[['update_numeric', 'instant_bookable', 'DSR', 'availability_365']]\n",
    "    \n",
    "\n",
    "    list_file.loc[:, 'DSR'] = checks['DSR']\n",
    "    list_file.loc[:, 'cal_trust'] = (((checks['update_numeric'] <= update_cutoff) | (checks['instant_bookable'] =='t') \n",
    "                                     | (checks['availability_365'] > avail_cutoff))*1) #      \n",
    "    trusted_ids = list_file[list_file['cal_trust'] == 1]['id'].unique()\n",
    "    \n",
    "    list_file['price'] = list_file.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "     # Reduce the size of variables in the listings dataframe.\n",
    "    list_file_small = list_file[['id', 'price', 'host_listings_count','bedrooms', 'bathrooms', \n",
    "                                 'neighbourhood', 'zipcode', 'room_type', 'instant_bookable', \n",
    "                                 'update_numeric', 'DSR', 'cal_trust', 'availability_365']]\n",
    "    \n",
    "    list_file_small.columns = ['id', 'headline_price', 'host_lists', 'bedrooms', \n",
    "                               'bathrooms', 'neigh', 'zip', 'type', 'instant', \n",
    "                               'DSupdate', 'DSReview', 'Active_flag', 'avail365']\n",
    "    # Managing calendar dataframe\n",
    "    cal_file = pd.read_csv(\"united-states_portland_\" + date + \"_calendar.csv.gz\", compression = 'gzip')\n",
    "\n",
    "    # This is needed because newer calendar files add cols\n",
    "    cal_file = cal_file[['listing_id', 'date','available', 'price']] \n",
    "\n",
    "    # Deal with different scrape times\n",
    "    cal_file = cal_file[(cal_file['date'] >= cal_start)& (cal_file['date'] <= cal_end)] \n",
    "    \n",
    "    # Prices converted to floats\n",
    "    cal_file['price'] = cal_file.price.replace('[\\$,]', '', regex=True).astype(float)   \n",
    "    \n",
    "    print(\"----Cleaning calendar and listings data for \" + date + \"----\")\n",
    "    print(\"Listing-dates removed: \" + str(float(len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))))\n",
    "    print(\"Unique listings removed: \" + str(float((len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))/365)))\n",
    "    print(\"                                                           \")\n",
    "    \n",
    "    cal_file = cal_file[cal_file['listing_id'].isin(trusted_ids)] # Removes questionable calendar data\n",
    "    # Rename columns\n",
    "    cal_file.columns = ['id', 'date', 'avail', 'night_price']\n",
    "    \n",
    "    return cal_file, list_file_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one scrape date in file indexed 15!\n",
      "['2017-03-06T00:00:00.000000000' '2017-03-05T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 16!\n",
      "['2017-04-07T00:00:00.000000000' '2017-04-08T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 25!\n",
      "['2018-01-16T00:00:00.000000000' '2018-01-17T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 27!\n",
      "['2018-04-11T00:00:00.000000000' '2018-04-12T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 28!\n",
      "['2018-05-14T00:00:00.000000000' '2018-05-13T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 32!\n",
      "['2018-10-09T00:00:00.000000000' '2018-10-12T00:00:00.000000000']\n",
      "More than one scrape date in file indexed 33!\n",
      "['2018-11-07T00:00:00.000000000' '2018-11-09T00:00:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "# This loop simply identifies when a scrape took more than one day, \n",
    "# and creates an adjustment layer based on this information.\n",
    "\n",
    "os.chdir(csv_path)\n",
    "i = 0\n",
    "adjustment_layer = []\n",
    "for date in init_dates:\n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv.gz\", low_memory = False, parse_dates = dateCols)\n",
    "\n",
    "    if len(list_file.last_scraped.unique()) > 1:\n",
    "        \n",
    "        print(\"More than one scrape date in file indexed \" + str(i) + \"!\")\n",
    "        print(list_file.last_scraped.unique())\n",
    "        adjustment_layer.append(len(list_file.last_scraped.unique()))\n",
    "    else:\n",
    "#         print(\"*No* issue with file index \" + str(i))\n",
    "        adjustment_layer.append(0)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Cleaning calendar and listings data for 2015-09-02----\n",
      "Listing-dates removed: 730.0\n",
      "Unique listings removed: 2.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-11-02----\n",
      "Listing-dates removed: 1825.0\n",
      "Unique listings removed: 5.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-12-02----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-01-01----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-02-03----\n",
      "Listing-dates removed: 25116.0\n",
      "Unique listings removed: 68.81095890410958\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-04-05----\n",
      "Listing-dates removed: 41610.0\n",
      "Unique listings removed: 114.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-05-03----\n",
      "Listing-dates removed: 45990.0\n",
      "Unique listings removed: 126.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-06-03----\n",
      "Listing-dates removed: 47815.0\n",
      "Unique listings removed: 131.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-07-04----\n",
      "Listing-dates removed: 49640.0\n",
      "Unique listings removed: 136.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-08-04----\n",
      "Listing-dates removed: 47815.0\n",
      "Unique listings removed: 131.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-09-04----\n",
      "Listing-dates removed: 49275.0\n",
      "Unique listings removed: 135.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-11-06----\n",
      "Listing-dates removed: 65700.0\n",
      "Unique listings removed: 180.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-12-08----\n",
      "Listing-dates removed: 70980.0\n",
      "Unique listings removed: 194.46575342465752\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-01-04----\n",
      "Listing-dates removed: 78840.0\n",
      "Unique listings removed: 216.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-02-09----\n",
      "Listing-dates removed: 81364.0\n",
      "Unique listings removed: 222.9150684931507\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-03-05----\n",
      "Listing-dates removed: 86031.0\n",
      "Unique listings removed: 235.7013698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-04-07----\n",
      "Listing-dates removed: 92202.0\n",
      "Unique listings removed: 252.60821917808218\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-05-07----\n",
      "Listing-dates removed: 92198.0\n",
      "Unique listings removed: 252.5972602739726\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-06-05----\n",
      "Listing-dates removed: 88481.0\n",
      "Unique listings removed: 242.413698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-07-06----\n",
      "Listing-dates removed: 86870.0\n",
      "Unique listings removed: 238.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-08-06----\n",
      "Listing-dates removed: 89180.0\n",
      "Unique listings removed: 244.32876712328766\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-09-12----\n",
      "Listing-dates removed: 93805.0\n",
      "Unique listings removed: 257.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-10-04----\n",
      "Listing-dates removed: 104390.0\n",
      "Unique listings removed: 286.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-11-13----\n",
      "Listing-dates removed: 113150.0\n",
      "Unique listings removed: 310.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-12-09----\n",
      "Listing-dates removed: 127400.0\n",
      "Unique listings removed: 349.041095890411\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-01-16----\n",
      "Listing-dates removed: 139029.0\n",
      "Unique listings removed: 380.9013698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-02-08----\n",
      "Listing-dates removed: 139611.0\n",
      "Unique listings removed: 382.4958904109589\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-04-11----\n",
      "Listing-dates removed: 154638.0\n",
      "Unique listings removed: 423.6657534246575\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-05-13----\n",
      "Listing-dates removed: 151734.0\n",
      "Unique listings removed: 415.7095890410959\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-07-10----\n",
      "Listing-dates removed: 152935.0\n",
      "Unique listings removed: 419.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-08-14----\n",
      "Listing-dates removed: 155490.0\n",
      "Unique listings removed: 426.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-09-14----\n",
      "Listing-dates removed: 157680.0\n",
      "Unique listings removed: 432.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-10-09----\n",
      "Listing-dates removed: 158994.0\n",
      "Unique listings removed: 435.6\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-11-07----\n",
      "Listing-dates removed: 165165.0\n",
      "Unique listings removed: 452.5068493150685\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-12-10----\n",
      "Listing-dates removed: 166075.0\n",
      "Unique listings removed: 455.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2019-01-13----\n",
      "Listing-dates removed: 174356.0\n",
      "Unique listings removed: 477.6876712328767\n",
      "                                                           \n"
     ]
    }
   ],
   "source": [
    "# This section creates a list of calendar and listing frames for each date range.\n",
    "\n",
    "dates_in_datetime = init_dates.astype('datetime64[D]')\n",
    "\n",
    "my_cals = []\n",
    "my_lists = []\n",
    "for i in range (0, len(dates_in_datetime)-1):\n",
    "    if (adjustment_layer[i]!=0):\n",
    "        cal, listing = complex_extract_cal_list(str(dates_in_datetime[i]), \n",
    "                                                str(dates_in_datetime[i] + adjustment_layer[i]), \n",
    "                                                str(dates_in_datetime[i] + 365 - (adjustment_layer[i]-1)))    \n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)\n",
    "    else:\n",
    "        cal, listing = complex_extract_cal_list(str(dates_in_datetime[i]), \n",
    "                                                str(dates_in_datetime[i] + adjustment_layer[i]), \n",
    "                                                str(dates_in_datetime[i] + 364))\n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Select listings ids and dates of interest\n",
    "\n",
    "Appropriate listings and dates will depend on a researcher’s specific focus, here ids are chosen based on listings are selected based on criteria set-up in a previous notebook and dates are set to avoid 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df[(listings_df['scrape_batch'] >= str(dates_in_datetime[0])) \n",
    "                          & (listings_df['scrape_batch'] <= str(dates_in_datetime[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8410 1582\n"
     ]
    }
   ],
   "source": [
    "# Focus on ids that have been kept after listings data cleaning process (listings_df)\n",
    "\n",
    "ids = []\n",
    "unq_dates = []\n",
    "\n",
    "for cal in my_cals:\n",
    "    ids.append(cal['id'].unique())\n",
    "    unq_dates.append(cal['date'].unique())\n",
    "\n",
    "all_ids = np.sort(np.unique(np.concatenate(ids)))    \n",
    "all_dates = np.sort(np.unique(np.concatenate(unq_dates)))\n",
    "\n",
    "listings_df = listings_df[listings_df['drop_indicator'] == 0]\n",
    "\n",
    "# See if the data cleaning dropped any of the selected listings:\n",
    "id_mask = np.isin(all_ids, listings_df['id'].unique())\n",
    "\n",
    "# Could Avoid 2020 if worried about COVID-19\n",
    "# date_mask = all_dates < '2020-01-01' \n",
    "\n",
    "all_ids = all_ids[id_mask]\n",
    "all_dates = all_dates[date_mask]\n",
    "\n",
    "print(len(all_ids), len(all_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to identify bookings and their associated prices\n",
    "\n",
    "Function is created to identify bookings and prices for all listings in the selected sample. This function leverages numpy arrays to improve efficiency. The larger these numpy arrays become, the greater the strain on a computer’s memory. It will usually be necessary to process data in separate chunks.\n",
    "\n",
    "There are two ways in which a property is considered booked:\n",
    "\n",
    "* **Available to unavailable:** \n",
    "     Firstly a property is booked if the sum of the avail_change column for a \n",
    "     given property across various scrapes is equal to -1. This represents the \n",
    "     fact that a property was seen as available and then it became unavailable. Going from avail = 1 to avail = 0 \n",
    "     means that we subract 1 for this variable.\n",
    " \n",
    "* **Allowing for unavailable to available to unavailable:** \n",
    "    I want to allow for a property to be \n",
    "    unavailable, become available and then switch \n",
    "    back to being unavailable. To achieve this, I \n",
    "    set three conditions:\n",
    "\n",
    "    * **Cond 2.1:** Sum of the avail_change row is 0, \n",
    "    this allows for a property to go \n",
    "    from unavailable to available (1) \n",
    "    and then go from available to \n",
    "    unavailable (-1)\n",
    "\n",
    "    * **Cond 2.2:** Second, the last availability seen \n",
    "    must be equal to zero\n",
    "\n",
    "    * **Cond 2.3:** Thirdly, the property must have been \n",
    "    seen as available at some point so we\n",
    "    must have a 1 in the avail_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_file(df, unq_dates, unq_ids):\n",
    "\n",
    "    \"\"\"\n",
    "    Function ensures that the calendar file \n",
    "    only contains ids and dates chosen by the researcher.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df[(df['date'].isin(unq_dates)) & \n",
    "              (df['id'].isin(unq_ids))].sort_values(by=['id','date']).reset_index(drop = True)\n",
    "\n",
    "def highest_index(a):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function finds the highest index for an array that is not null.\n",
    "    If all values are null then return 99999.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return a[~np.isnan(a)][-1]\n",
    "    except: \n",
    "        return 99999\n",
    "    \n",
    "def last_nonzero(arr, axis, invalid_val=-1):\n",
    "    \"\"\"\n",
    "    Function finds the last non-zero value of an array.\n",
    "    \"\"\"\n",
    "\n",
    "    mask = arr!= \"nan\"\n",
    "    val = arr.shape[axis] - np.flip(mask,axis=axis).argmax(axis=axis) - 1\n",
    "\n",
    "    return np.where(mask.any(axis=axis), val, invalid_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(init=0,stop=100):\n",
    "    \"\"\"\n",
    "    This function produces and saves a final booked_df for ids within the range\n",
    "    init and stop in the all_ids array.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    unique_ids = all_ids[init:stop]\n",
    "    unique_dates = all_dates \n",
    "\n",
    "    # Save the number of ids and dates\n",
    "\n",
    "    N_IDS = len(unique_ids) \n",
    "    N_DATES = len(unique_dates) \n",
    "    key_list = []\n",
    "\n",
    "# =========================================================    \n",
    "\n",
    "    for an_id in unique_ids:\n",
    "        for a_date in unique_dates:\n",
    "            key_list.append(an_id.astype(str) + \":\" + a_date)\n",
    "    key_df = pd.DataFrame(key_list, columns=['key'])\n",
    "\n",
    "# =========================================================\n",
    "\n",
    "    calendar_scrapes = []\n",
    "\n",
    "    for my_cal in my_cals:\n",
    "        calendar_scrapes.append(cal_file(my_cal, unique_dates, unique_ids))\n",
    "\n",
    "    N_CALS = len(calendar_scrapes)\n",
    "    i = 1\n",
    "    \n",
    "    for cal_files in calendar_scrapes:\n",
    "        \n",
    "        cal_files.loc[:, 'key'] = cal_files['id'].astype(str).str.strip() + \":\" + cal_files['date'].str.strip()\n",
    "        cal_files.columns = ['id' + str(i), 'date' + str(i), 'avail' + str(i), 'night_price' + str(i),\n",
    "                        'key']   \n",
    "        i+=1\n",
    "\n",
    "# =========================================================\n",
    "\n",
    "# Create a dataframe for all availabilities and prices for every listing-night\n",
    "# across scrape files. \n",
    "#    * Note: This dataframe is broken up into separate availability and price matrices\n",
    "        \n",
    "    cal_dfs = [key_df]\n",
    "\n",
    "    for i in range(N_CALS):\n",
    "        count = i + 1 \n",
    "        cal_dfs.append(calendar_scrapes[i][[('avail' + str(count)), 'key', ('night_price' + str(count))]])\n",
    "    \n",
    "\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['key'],\n",
    "                                            how='outer'), cal_dfs).fillna(np.nan) \n",
    "\n",
    "    df_merged.loc[:, \"id\"], df_merged.loc[:, \"date\"] = (df_merged['key'].str.split(\":\", expand=True)[0].astype(int),\n",
    "                                              df_merged['key'].str.split(\":\", expand=True)[1])\n",
    "    df_merged = df_merged.drop(columns=['key'])\n",
    "\n",
    "    df_merged = df_merged.drop_duplicates()\n",
    "\n",
    "# =========================================================\n",
    "\n",
    "    # Create a list of columns with only availabilities and the id and date\n",
    "    avail_cols = df_merged.columns[df_merged.columns.str.contains('avail')]\n",
    "    avail_cols = np.append(avail_cols, ('id','date'))\n",
    "\n",
    "    avail_arr = df_merged[avail_cols].values\n",
    "\n",
    "    avail_matrix = np.reshape(avail_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F') \n",
    "\n",
    "    # Convert 't' and 'f' strings into binary floats\n",
    "    avail_matrix = np.where(avail_matrix == 't', 1., avail_matrix) \n",
    "    avail_matrix = np.where(avail_matrix == 'f', 0., avail_matrix)\n",
    "    \n",
    "    # Take the difference between availability status across scrape files\n",
    "    avail_change = np.diff(avail_matrix, axis=2)\n",
    "    \n",
    "# =========================================================\n",
    "    \n",
    "    case_1 = (np.nansum(avail_change, axis=2) == -1)*1 \n",
    "# =========================================================    \n",
    "\n",
    "    str_avail = avail_matrix.astype(str)\n",
    "    inds = last_nonzero(str_avail, axis=2, invalid_val=np.nan)\n",
    "\n",
    "    test_inds = inds\n",
    "    test_inds = np.reshape(test_inds, (N_DATES*N_IDS), order='F')\n",
    "\n",
    "# =========================================================\n",
    "    # Get the last availability for all properties\n",
    "    \n",
    "    case2_df = pd.DataFrame(test_inds)\n",
    "    case2_df.columns = ['cal_index']\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].astype('Int64')\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].fillna(0) \n",
    "    my_vals = case2_df['cal_index'].values \n",
    "\n",
    "    reshape_avail = np.reshape(avail_matrix, (N_DATES*N_IDS*N_CALS,), order='F')\n",
    "\n",
    "    case2_df['id_index'] = (case2_df.index/N_DATES).astype(int)\n",
    "    case2_df['date_index'] = case2_df.index - N_DATES*case2_df['id_index']\n",
    "\n",
    "\n",
    "    indices = (case2_df['cal_index']*N_DATES*N_IDS + case2_df['id_index']*N_DATES + case2_df['date_index']).values\n",
    "\n",
    "    case2_result = reshape_avail[indices.astype(int)]\n",
    "    case2_result = np.reshape(case2_result, (N_DATES, N_IDS), order='F').astype(float)\n",
    "    \n",
    "# =========================================================\n",
    "\n",
    "    cond_21 = (np.nansum(avail_change, axis=2) == 0)*1\n",
    "    cond_22 = (case2_result == 0)*1\n",
    "    cond_23 = (np.nanmax(avail_matrix,axis=2) == 1)*1\n",
    "\n",
    "    case_2 = cond_21*cond_22*cond_23\n",
    "    \n",
    "# =========================================================\n",
    "    # CREATE A MATRIX FOR BOOKINGS\n",
    "    \n",
    "    booked_mat = np.maximum(case_1, case_2)\n",
    "    booked_mat = np.reshape(booked_mat, (N_DATES*N_IDS,1), order='F')\n",
    "\n",
    "    booked_df = pd.DataFrame(booked_mat >= 1 , columns=['booked'])\n",
    "    booked_df.loc[:, \"id\"] = df_merged['id'].values\n",
    "    booked_df.loc[:, \"res_date\"] = df_merged['date'].values\n",
    "\n",
    "    never_avail = np.nanmax(avail_matrix,axis=2) != 1.\n",
    "    booked_df.loc[:, 'never_avail'] = np.reshape(never_avail, (N_DATES*N_IDS, 1), order='F')\n",
    "    \n",
    "# =========================================================\n",
    "    # CREATE A MATRIX FOR PRICES\n",
    "    \n",
    "    price_cols = df_merged.columns[df_merged.columns.str.contains('night_price')]\n",
    "    price_cols = np.append(price_cols, ('id','date'))\n",
    "\n",
    "    price_arr = df_merged[price_cols].values\n",
    "\n",
    "    price_matrix = np.reshape(price_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F')\n",
    "    \n",
    "# =========================================================\n",
    "    \n",
    "    last_avail_flag = (np.roll(avail_change,0, axis=2) == -1)*1\n",
    "    last_avail_flag = np.where(last_avail_flag == 0, np.nan, last_avail_flag)\n",
    "\n",
    "    booked_price = np.nanmax(price_matrix[:,:,0:(N_CALS - 1)]*last_avail_flag,axis=2)\n",
    "    booked_df.loc[:, 'price'] = np.reshape(booked_price, (N_DATES*N_IDS,1), order='F')\n",
    "    last_date_avail = np.isfinite(last_avail_flag).argmax(2)\n",
    "    index_for_last = np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')\n",
    "    last_date = np.array(init_dates)[np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')]\n",
    "\n",
    "    # This ensures that the last date is only reported whenever the listing is identified as being booked.\n",
    "    booked_df.loc[:, \"last_date\"] = last_date\n",
    "    mask = booked_df.booked == False\n",
    "    column_name = 'last_date'\n",
    "    booked_df.loc[mask, column_name] = np.nan\n",
    "    \n",
    "# =========================================================\n",
    "    \n",
    "    # Add prices for places that are available but not booked\n",
    "    price_matrix_np = np.float64(price_matrix)\n",
    "    \n",
    "    last_observed_price = np.apply_along_axis(highest_index, 2, price_matrix_np)\n",
    "    last_observed_price = np.where(last_observed_price == 99999, np.nan, last_observed_price)\n",
    "    last_observed_price = np.reshape(last_observed_price, (N_IDS*N_DATES), order='F')\n",
    "    \n",
    "# =========================================================\n",
    "\n",
    "    booked_df = booked_df.sort_values(by=['id', 'res_date'])\n",
    "    booked_df.loc[:, \"all_prices\"] =  last_observed_price\n",
    "    \n",
    "    # If booked, get assigned \"price\" if un-booked get assigned \"all_prices\". Final prices is the variable we care about\n",
    "    booked_ind = (booked_df['booked']*1).values\n",
    "\n",
    "    final_prices = ((booked_ind)*booked_df['price'].fillna(0) + (1 - booked_ind)*booked_df['all_prices'].fillna(0)).values\n",
    "    booked_df.loc[:, 'final_prices'] = final_prices\n",
    "    booked_df['final_prices'].replace({0.:np.nan}, inplace=True)\n",
    "    booked_df.loc[:, \"seen_avail\"] = (booked_df['never_avail'] == False).values*1\n",
    "    booked_df = booked_df.sort_values(by='res_date').reset_index(drop=True)    \n",
    "    booked_df.loc[:, 'week_yr'] =  booked_df['res_date'].astype('datetime64').dt.strftime('%Y-%U')\n",
    "    booked_df.loc[:, 'mo_yr'] = booked_df['res_date'].astype('datetime64').dt.to_period('M')\n",
    "    booked_df_save = booked_df.copy()\n",
    "    booked_df_save.loc[:, \"mo_yr\"] = booked_df_save[\"mo_yr\"].astype(str)\n",
    "    \n",
    "# =========================================================\n",
    "    # APPEND TO LIST\n",
    "    \n",
    "    os.chdir(data_path) #MOVE THIS !\n",
    "    df_list.append(booked_df_save)\n",
    "    \n",
    "#     print('chunk '+ str(j) + \" completed\") # Creates many prints, helpful for error checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing function Call and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100 # Chunk size determined through trial and error\n",
    "N_chunks = int(np.ceil(len(all_ids)/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "# Iterative loop to prevent kernel crash\n",
    "\n",
    "df_list = []\n",
    "\n",
    "j = 0\n",
    "for i in range(N_chunks):\n",
    "    process(init=i*chunk_size, stop=(i + 1)*chunk_size)\n",
    "    j+=1\n",
    "    \n",
    "booked_df_final = pd.concat(df_list)\n",
    "booked_df_final.to_csv(city_abbrev + '_rejoined_booked_df.csv.gz', \n",
    "                       index=False, compression='gzip')\n",
    "\n",
    "print(\"Process complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
