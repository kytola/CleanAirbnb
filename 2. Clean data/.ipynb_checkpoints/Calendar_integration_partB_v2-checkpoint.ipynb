{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar Integration partB\n",
    "\n",
    "This Jupyter notebook contains the second half of calendar integration and it produces a dataframe named \"cal_rev_list_FULLMERGEv3_10_days_before.csv\" which is then saved in the Saved Data folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Universal Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable garbage collection module for memory purposes\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/'\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many days before a review a property is assumed to be active. Important assumption!\n",
    "N_days_before = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-03-05', '2018-01-16', '2017-11-13', '2018-04-11', '2016-04-05', '2017-09-12', '2015-05-12', '2017-10-04', '2016-11-06', '2018-12-10', '2017-07-06', '2018-05-13', '2016-08-04', '2018-02-08', '2015-03-01', '2017-01-04', '2017-08-06', '2016-01-01', '2018-11-07', '2019-01-13', '2015-12-02', '2016-12-08', '2015-11-02']\n"
     ]
    }
   ],
   "source": [
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)      \n",
    "\n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "init_dates = file_dates[3:26]\n",
    "\n",
    "print(init_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload listings dataframe/cross sectional file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "\n",
    "listings_df = pd.read_csv('2ndStageClean_Portland.csv.gz', compression = 'gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add free parking and pool amenities\n",
    "parking_search = ['Free parking on premises']\n",
    "listings_df.loc[:, 'free_park'] = (listings_df['amenities'].str.contains('|'.join(parking_search), na=False)*1).values\n",
    "\n",
    "pool_search = ['pool', 'Pool']\n",
    "listings_df.loc[:, 'pool'] = (listings_df['amenities'].str.contains('|'.join(pool_search), na=False)*1).values\n",
    "\n",
    "os.chdir(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the booked_df that is created by previous Jupyter Notebook\n",
    "\n",
    "os.chdir(data_path)\n",
    "booked_df = pd.read_csv('rejoined_booked_df.csv')\n",
    "booked_df['date_notstring'] = booked_df['date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating review files\n",
    "\n",
    "If a property receives a review then I assume that it was available for the a certain number of days prior to that review.\n",
    "\n",
    "The commented out code in this section creates the review dataframe as long as you have all of the review scrapes in the \"csv_path\" folder. After it has been created, all that needs to be done is to load it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Collect review csv names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect reviews csvs names\n",
    "\n",
    "os.chdir(csv_path)\n",
    "revFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"reviews.csv.gz\"):\n",
    "        revFiles.append(fileNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create a function to concatenate review dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " def concat_spreadsheets(START, END):\n",
    "    sheets_df = []\n",
    "    \n",
    "    for filename in revFiles[START:END]:\n",
    "        df = pd.read_csv(filename, index_col = None, header=0)\n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Run the function and save a compressed dataframe with all unique reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6635700 496910\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE, THIS CREATES THE AGGREGATE REVIEW FILE\n",
    "\n",
    "collected_revs = concat_spreadsheets(3, len(revFiles))\n",
    "uniq_revs = collected_revs.drop_duplicates()\n",
    "print(len(collected_revs), len(uniq_revs))\n",
    "uniq_revs.to_csv('Revs_Portland.csv.gz', compression='gzip', index=False) \n",
    "#This creates the compressed revdate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Revs_Portland.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86423/1012490435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0muniq_revs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Revs_Portland.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                 handle = gzip.GzipFile(\n\u001b[0m\u001b[1;32m    643\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Revs_Portland.csv.gz'"
     ]
    }
   ],
   "source": [
    "# This cell imports the compressed review datafame collected above. Update the filename \n",
    "# accordingly when additional dates are added. \n",
    "\n",
    "#os.chdir(data_path)\n",
    "\n",
    "#uniq_revs = pd.read_csv('Revs_Portland.csv.gz', compression='gzip')\n",
    "\n",
    "#os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the listing IDs that are not in the calendar files\n",
    "unique_ids = booked_df.id.unique()\n",
    "uniq_revs = uniq_revs[uniq_revs.listing_id.isin(unique_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natfern/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5716591</td>\n",
       "      <td>2015-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5716591</td>\n",
       "      <td>2015-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5716591</td>\n",
       "      <td>2015-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5716591</td>\n",
       "      <td>2015-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5716591</td>\n",
       "      <td>2015-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id       date\n",
       "9      5716591 2015-09-02\n",
       "10     5716591 2015-09-07\n",
       "11     5716591 2015-09-13\n",
       "12     5716591 2015-09-19\n",
       "13     5716591 2015-09-22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_min, date_max = booked_df['date'].min(), booked_df['date'].max() # Use the dates in my calendar data files\n",
    "\n",
    "revs_df = uniq_revs[['listing_id', 'date']]\n",
    "\n",
    "revs_df.loc[:, 'date'] = revs_df['date'].astype('datetime64')\n",
    "mask = (revs_df['date'] >= date_min) & (revs_df['date'] <= date_max)\n",
    "revs_df = revs_df[mask]\n",
    "revs_short = revs_df \n",
    "\n",
    "# Dataframe of dates\n",
    "reviews_file = revs_short\n",
    "N_reviews = len(reviews_file)\n",
    "reviews_file.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active property dataframe creation\n",
    "\n",
    "Much like the review code, the commented out block below creates a pandas dataframe that tracks the dates during which a property is active.\n",
    "\n",
    "Once the dataframe is created it can just be loaded in, meaning you can comment out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389479\n"
     ]
    }
   ],
   "source": [
    "# Here I identify the unique dates from the reviews_file and identify the \n",
    "# index for every row of the reviews_file that maps to the unique dates file. \n",
    "# This allows me to avoid duplicate calculations for identical dates.\n",
    "\n",
    "\n",
    "my_dates = reviews_file['date'].astype('datetime64[D]')\n",
    "\n",
    "unq_dates = pd.DataFrame([my_dates.unique()]).T\n",
    "unq_dates.columns = ['unq_dates']\n",
    "unq_dates = unq_dates.sort_values(by='unq_dates').reset_index(drop=True)\n",
    "\n",
    "# # Identify the valid index in the unq_arr for the non-unique list of dates\n",
    "\n",
    "dates_arr = my_dates.values\n",
    "unq_arr = np.concatenate(unq_dates.values)\n",
    "\n",
    "sorter = np.argsort(unq_arr)\n",
    "my_indices = sorter[np.searchsorted(unq_arr, dates_arr, sorter=sorter)]\n",
    "\n",
    "my_dates = pd.DataFrame(my_dates.values, columns = ['dates'])\n",
    "my_dates.loc[:, \"unq_ind\"] = my_indices\n",
    "unq_dates.loc[np.r_[my_dates['unq_ind']], :] # Don't delete! This lets me explore what is going on\n",
    "\n",
    "# # Here I create all of the date ranges that I want to work with.\n",
    "\n",
    "unq_dates.loc[:,\"start\"] = unq_dates['unq_dates'] - datetime.timedelta(days=N_days_before)\n",
    "\n",
    "days_before = []\n",
    "\n",
    "for i in range(len(unq_dates)):\n",
    "      days_before.append(pd.date_range(unq_dates['start'][i], unq_dates['unq_dates'][i], closed='left'))\n",
    "    \n",
    "# #  ------ Here I create a dataframe that has all of the listing, takes at least 4.5 minutes ------------------------------\n",
    "\n",
    "active_dates = np.concatenate(np.array(days_before)[my_dates['unq_ind'].values].astype('datetime64[D]'))\n",
    "my_date_ids = np.reshape(pd.concat([reviews_file['listing_id']]*N_days_before, axis=1).values, (N_days_before*N_reviews,1))\n",
    "my_date_ids = np.concatenate(my_date_ids)\n",
    "\n",
    "active_df = pd.DataFrame([my_date_ids, active_dates]).T\n",
    "active_df.columns = ['id', 'date']\n",
    "active_df.loc[:, \"key\"] = active_df['id'].astype(str) + \":\" + active_df['date'].astype(str)\n",
    "\n",
    "print(N_reviews)\n",
    "active_df.head(5)\n",
    "\n",
    "active_df.to_csv('active_dates'+ str(N_days_before) + \".csv.gz\", compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_active_file is created by the code that is commented out below! \n",
    "\n",
    "#my_active_file = \"active_dates\" + str(N_days_before) + \".csv.gz\"\n",
    "\n",
    "#active_df = pd.read_csv(my_active_file, compression='gzip', parse_dates=['date'])\n",
    "#active_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-merge processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df['id_str']= booked_df['id'].astype(str)\n",
    "booked_df.loc[:, \"key\"] = booked_df['id_str'] + \":\" + booked_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df.loc[:, 'mo_yr'] = booked_df['date_notstring'].dt.to_period('M')\n",
    "booked_df.loc[:, \"id_mo_key\"] = booked_df['mo_yr'].astype(str) + \":\" + booked_df['id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df['batch_YRMO'] = listings_df['batch_YRMO'].astype('datetime64').dt.to_period('M')\n",
    "listings_mask = (listings_df['batch_YRMO'] >= booked_df['mo_yr'].min()) & (listings_df['batch_YRMO'] <= booked_df['mo_yr'].max())\n",
    "listings_df = listings_df[listings_mask]\n",
    "listings_df.loc[:, \"id_mo_key\"] = listings_df['batch_YRMO'].astype(str) + \":\" + listings_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df = booked_df.rename(columns={'price':\"calendar_price\"})\n",
    "# booked_df = booked_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df.rename(columns={'price':\"headline_price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge time-varying variables from the panel to the booked_df dataframe. Prices here are \"headline prices\".\n",
    "\n",
    "list_month_merge = booked_df.merge(listings_df[['id_mo_key', 'List_month', 'Listlead1', \n",
    "                                                'Listlag1', 'headline_price', 'cleaning_fee', 'host_listings_count', 'cum_sum',\n",
    "                                                 'bedrooms', 'room_type', 'neighbourhood', 'zipcode', # I wanted to have this last row separte \n",
    "                                               'free_park', 'pool', 'host_since', 'review_scores_rating', 'number_of_reviews']], # These two are new here\n",
    "                                   left_on='id_mo_key', right_on='id_mo_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some memory\n",
    "\n",
    "del booked_df\n",
    "del listings_df\n",
    "del revs_df\n",
    "del listings_mask\n",
    "del revs_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5577533\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>book_status</th>\n",
       "      <th>seen_avail</th>\n",
       "      <th>calendar_price</th>\n",
       "      <th>date_notstring</th>\n",
       "      <th>id_str</th>\n",
       "      <th>key</th>\n",
       "      <th>mo_yr</th>\n",
       "      <th>...</th>\n",
       "      <th>cum_sum</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>room_type</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>free_park</th>\n",
       "      <th>pool</th>\n",
       "      <th>host_since</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>number_of_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9356</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>9356</td>\n",
       "      <td>9356:2015-09-02</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Woodlawn</td>\n",
       "      <td>97211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-08-06 00:00:00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>156479</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>156479</td>\n",
       "      <td>156479:2015-09-02</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>97203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-08-10 00:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41601</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>41601</td>\n",
       "      <td>41601:2015-09-02</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Roseway</td>\n",
       "      <td>97213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-07-24 00:00:00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>240583</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>240583</td>\n",
       "      <td>240583:2015-09-02</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Mt. Tabor</td>\n",
       "      <td>97215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-07-16 00:00:00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>218708</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>218708</td>\n",
       "      <td>218708:2015-09-02</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>Kerns</td>\n",
       "      <td>97232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-05-09 00:00:00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id        date  book_status  seen_avail  calendar_price  \\\n",
       "0           0    9356  2015-09-02            0           1            70.0   \n",
       "1           2  156479  2015-09-02            0           1            60.0   \n",
       "2           3   41601  2015-09-02            0           1           109.0   \n",
       "3           7  240583  2015-09-02            0           1            95.0   \n",
       "4          10  218708  2015-09-02            0           1           155.0   \n",
       "\n",
       "  date_notstring  id_str                key    mo_yr  ... cum_sum  bedrooms  \\\n",
       "0     2015-09-02    9356    9356:2015-09-02  2015-09  ...     1.0       2.0   \n",
       "1     2015-09-02  156479  156479:2015-09-02  2015-09  ...     2.0       1.0   \n",
       "2     2015-09-02   41601   41601:2015-09-02  2015-09  ...     2.0       1.0   \n",
       "3     2015-09-02  240583  240583:2015-09-02  2015-09  ...    17.0       1.0   \n",
       "4     2015-09-02  218708  218708:2015-09-02  2015-09  ...     4.0       1.0   \n",
       "\n",
       "         room_type  neighbourhood  zipcode  free_park  pool  \\\n",
       "0  Entire home/apt       Woodlawn    97211        0.0   0.0   \n",
       "1     Private room     Portsmouth    97203        0.0   0.0   \n",
       "2  Entire home/apt        Roseway    97213        0.0   0.0   \n",
       "3  Entire home/apt      Mt. Tabor    97215        0.0   0.0   \n",
       "4  Entire home/apt          Kerns    97232        0.0   0.0   \n",
       "\n",
       "            host_since  review_scores_rating number_of_reviews  \n",
       "0  2009-08-06 00:00:00                  97.0              57.0  \n",
       "1  2009-08-10 00:00:00                  95.0               4.0  \n",
       "2  2010-07-24 00:00:00                  96.0             106.0  \n",
       "3  2011-07-16 00:00:00                  98.0              10.0  \n",
       "4  2013-05-09 00:00:00                  97.0              94.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_month_merge))\n",
    "list_month_merge.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 completed\n",
      "chunk 1 completed\n",
      "chunk 2 completed\n",
      "chunk 3 completed\n",
      "chunk 4 completed\n",
      "chunk 5 completed\n",
      "chunk 6 completed\n",
      "chunk 7 completed\n",
      "chunk 8 completed\n",
      "chunk 9 completed\n",
      "chunk 10 completed\n",
      "chunk 11 completed\n",
      "chunk 12 completed\n",
      "chunk 13 completed\n",
      "chunk 14 completed\n",
      "chunk 15 completed\n",
      "chunk 16 completed\n",
      "chunk 17 completed\n",
      "chunk 18 completed\n",
      "chunk 19 completed\n",
      "chunk 20 completed\n",
      "chunk 21 completed\n",
      "chunk 22 completed\n",
      "chunk 23 completed\n",
      "chunk 24 completed\n",
      "chunk 25 completed\n",
      "chunk 26 completed\n",
      "chunk 27 completed\n",
      "chunk 28 completed\n",
      "chunk 29 completed\n",
      "chunk 30 completed\n",
      "chunk 31 completed\n",
      "chunk 32 completed\n",
      "chunk 33 completed\n",
      "chunk 34 completed\n",
      "chunk 35 completed\n",
      "chunk 36 completed\n",
      "chunk 37 completed\n",
      "chunk 38 completed\n",
      "chunk 39 completed\n",
      "chunk 40 completed\n",
      "chunk 41 completed\n",
      "chunk 42 completed\n",
      "chunk 43 completed\n",
      "chunk 44 completed\n",
      "chunk 45 completed\n",
      "chunk 46 completed\n",
      "chunk 47 completed\n",
      "chunk 48 completed\n",
      "chunk 49 completed\n",
      "chunk 50 completed\n",
      "chunk 51 completed\n",
      "chunk 52 completed\n",
      "chunk 53 completed\n",
      "chunk 54 completed\n",
      "chunk 55 completed\n",
      "chunk 56 completed\n",
      "chunk 57 completed\n",
      "chunk 58 completed\n",
      "chunk 59 completed\n",
      "chunk 60 completed\n",
      "chunk 61 completed\n",
      "chunk 62 completed\n",
      "chunk 63 completed\n",
      "chunk 64 completed\n",
      "chunk 65 completed\n",
      "chunk 66 completed\n",
      "chunk 67 completed\n",
      "chunk 68 completed\n",
      "chunk 69 completed\n",
      "chunk 70 completed\n",
      "chunk 71 completed\n",
      "chunk 72 completed\n",
      "chunk 73 completed\n",
      "chunk 74 completed\n",
      "chunk 75 completed\n",
      "chunk 76 completed\n",
      "chunk 77 completed\n",
      "chunk 78 completed\n",
      "chunk 79 completed\n",
      "chunk 80 completed\n",
      "chunk 81 completed\n",
      "chunk 82 completed\n",
      "chunk 83 completed\n",
      "chunk 84 completed\n",
      "chunk 85 completed\n",
      "chunk 86 completed\n",
      "chunk 87 completed\n",
      "chunk 88 completed\n",
      "chunk 89 completed\n",
      "chunk 90 completed\n",
      "chunk 91 completed\n",
      "chunk 92 completed\n",
      "chunk 93 completed\n",
      "chunk 94 completed\n",
      "chunk 95 completed\n",
      "chunk 96 completed\n",
      "chunk 97 completed\n",
      "chunk 98 completed\n",
      "chunk 99 completed\n",
      "chunk 100 completed\n",
      "chunk 101 completed\n",
      "chunk 102 completed\n",
      "chunk 103 completed\n",
      "chunk 104 completed\n",
      "chunk 105 completed\n",
      "chunk 106 completed\n",
      "chunk 107 completed\n",
      "chunk 108 completed\n",
      "chunk 109 completed\n",
      "chunk 110 completed\n",
      "chunk 111 completed\n",
      "chunk 112 completed\n",
      "chunk 113 completed\n",
      "chunk 114 completed\n",
      "chunk 115 completed\n",
      "chunk 116 completed\n",
      "chunk 117 completed\n",
      "chunk 118 completed\n",
      "chunk 119 completed\n",
      "chunk 120 completed\n",
      "chunk 121 completed\n",
      "chunk 122 completed\n",
      "chunk 123 completed\n",
      "chunk 124 completed\n",
      "chunk 125 completed\n",
      "chunk 126 completed\n",
      "chunk 127 completed\n",
      "chunk 128 completed\n",
      "chunk 129 completed\n",
      "chunk 130 completed\n",
      "chunk 131 completed\n",
      "chunk 132 completed\n",
      "chunk 133 completed\n",
      "chunk 134 completed\n",
      "chunk 135 completed\n",
      "chunk 136 completed\n",
      "chunk 137 completed\n",
      "chunk 138 completed\n",
      "chunk 139 completed\n",
      "chunk 140 completed\n",
      "chunk 141 completed\n",
      "chunk 142 completed\n",
      "chunk 143 completed\n",
      "chunk 144 completed\n",
      "chunk 145 completed\n",
      "chunk 146 completed\n",
      "chunk 147 completed\n",
      "chunk 148 completed\n",
      "chunk 149 completed\n",
      "chunk 150 completed\n",
      "chunk 151 completed\n",
      "chunk 152 completed\n",
      "chunk 153 completed\n",
      "chunk 154 completed\n",
      "chunk 155 completed\n",
      "chunk 156 completed\n",
      "chunk 157 completed\n",
      "chunk 158 completed\n",
      "chunk 159 completed\n",
      "chunk 160 completed\n",
      "chunk 161 completed\n",
      "chunk 162 completed\n",
      "chunk 163 completed\n",
      "chunk 164 completed\n",
      "chunk 165 completed\n",
      "chunk 166 completed\n",
      "chunk 167 completed\n",
      "chunk 168 completed\n",
      "chunk 169 completed\n",
      "chunk 170 completed\n",
      "chunk 171 completed\n",
      "chunk 172 completed\n",
      "chunk 173 completed\n",
      "chunk 174 completed\n",
      "chunk 175 completed\n",
      "chunk 176 completed\n",
      "chunk 177 completed\n",
      "chunk 178 completed\n",
      "chunk 179 completed\n",
      "chunk 180 completed\n",
      "chunk 181 completed\n",
      "chunk 182 completed\n",
      "chunk 183 completed\n",
      "chunk 184 completed\n",
      "chunk 185 completed\n",
      "chunk 186 completed\n",
      "chunk 187 completed\n",
      "chunk 188 completed\n",
      "chunk 189 completed\n",
      "chunk 190 completed\n",
      "chunk 191 completed\n",
      "chunk 192 completed\n",
      "chunk 193 completed\n",
      "chunk 194 completed\n",
      "chunk 195 completed\n",
      "chunk 196 completed\n",
      "chunk 197 completed\n",
      "chunk 198 completed\n",
      "chunk 199 completed\n",
      "chunk 200 completed\n",
      "chunk 201 completed\n",
      "chunk 202 completed\n",
      "chunk 203 completed\n",
      "chunk 204 completed\n",
      "chunk 205 completed\n",
      "chunk 206 completed\n",
      "chunk 207 completed\n",
      "chunk 208 completed\n",
      "chunk 209 completed\n",
      "chunk 210 completed\n",
      "chunk 211 completed\n",
      "chunk 212 completed\n",
      "chunk 213 completed\n",
      "chunk 214 completed\n",
      "chunk 215 completed\n",
      "chunk 216 completed\n",
      "chunk 217 completed\n",
      "chunk 218 completed\n",
      "chunk 219 completed\n",
      "chunk 220 completed\n",
      "chunk 221 completed\n",
      "chunk 222 completed\n",
      "chunk 223 completed\n",
      "chunk 224 completed\n",
      "chunk 225 completed\n",
      "chunk 226 completed\n",
      "chunk 227 completed\n",
      "chunk 228 completed\n",
      "chunk 229 completed\n",
      "chunk 230 completed\n",
      "chunk 231 completed\n",
      "chunk 232 completed\n",
      "chunk 233 completed\n",
      "chunk 234 completed\n",
      "chunk 235 completed\n",
      "chunk 236 completed\n",
      "chunk 237 completed\n",
      "chunk 238 completed\n",
      "chunk 239 completed\n",
      "chunk 240 completed\n",
      "chunk 241 completed\n",
      "chunk 242 completed\n",
      "chunk 243 completed\n",
      "chunk 244 completed\n",
      "chunk 245 completed\n",
      "chunk 246 completed\n",
      "chunk 247 completed\n",
      "chunk 248 completed\n",
      "chunk 249 completed\n",
      "chunk 250 completed\n",
      "chunk 251 completed\n",
      "chunk 252 completed\n",
      "chunk 253 completed\n",
      "chunk 254 completed\n",
      "chunk 255 completed\n",
      "chunk 256 completed\n",
      "chunk 257 completed\n",
      "chunk 258 completed\n",
      "chunk 259 completed\n",
      "chunk 260 completed\n",
      "chunk 261 completed\n",
      "chunk 262 completed\n",
      "chunk 263 completed\n",
      "chunk 264 completed\n",
      "chunk 265 completed\n",
      "chunk 266 completed\n",
      "chunk 267 completed\n",
      "chunk 268 completed\n",
      "chunk 269 completed\n",
      "chunk 270 completed\n",
      "chunk 271 completed\n",
      "chunk 272 completed\n",
      "chunk 273 completed\n",
      "chunk 274 completed\n",
      "chunk 275 completed\n",
      "chunk 276 completed\n",
      "chunk 277 completed\n",
      "chunk 278 completed\n",
      "chunk 279 completed\n",
      "chunk 280 completed\n",
      "chunk 281 completed\n",
      "chunk 282 completed\n",
      "chunk 283 completed\n",
      "chunk 284 completed\n",
      "chunk 285 completed\n",
      "chunk 286 completed\n",
      "chunk 287 completed\n",
      "chunk 288 completed\n",
      "chunk 289 completed\n",
      "chunk 290 completed\n",
      "chunk 291 completed\n",
      "chunk 292 completed\n",
      "chunk 293 completed\n",
      "chunk 294 completed\n",
      "chunk 295 completed\n",
      "chunk 296 completed\n",
      "chunk 297 completed\n",
      "chunk 298 completed\n",
      "chunk 299 completed\n",
      "chunk 300 completed\n",
      "chunk 301 completed\n",
      "chunk 302 completed\n",
      "chunk 303 completed\n",
      "chunk 304 completed\n",
      "chunk 305 completed\n",
      "chunk 306 completed\n",
      "chunk 307 completed\n",
      "chunk 308 completed\n",
      "chunk 309 completed\n",
      "chunk 310 completed\n",
      "chunk 311 completed\n",
      "chunk 312 completed\n",
      "chunk 313 completed\n",
      "chunk 314 completed\n",
      "chunk 315 completed\n",
      "chunk 316 completed\n",
      "chunk 317 completed\n",
      "chunk 318 completed\n",
      "chunk 319 completed\n",
      "chunk 320 completed\n",
      "chunk 321 completed\n",
      "chunk 322 completed\n",
      "chunk 323 completed\n",
      "chunk 324 completed\n",
      "chunk 325 completed\n",
      "chunk 326 completed\n",
      "chunk 327 completed\n",
      "chunk 328 completed\n",
      "chunk 329 completed\n",
      "chunk 330 completed\n",
      "chunk 331 completed\n",
      "chunk 332 completed\n",
      "chunk 333 completed\n",
      "chunk 334 completed\n",
      "chunk 335 completed\n",
      "chunk 336 completed\n",
      "chunk 337 completed\n",
      "chunk 338 completed\n",
      "chunk 339 completed\n",
      "chunk 340 completed\n",
      "chunk 341 completed\n",
      "chunk 342 completed\n",
      "chunk 343 completed\n",
      "chunk 344 completed\n",
      "chunk 345 completed\n",
      "chunk 346 completed\n",
      "chunk 347 completed\n",
      "chunk 348 completed\n",
      "chunk 349 completed\n",
      "chunk 350 completed\n",
      "chunk 351 completed\n",
      "chunk 352 completed\n",
      "chunk 353 completed\n",
      "chunk 354 completed\n",
      "chunk 355 completed\n",
      "chunk 356 completed\n",
      "chunk 357 completed\n",
      "chunk 358 completed\n",
      "chunk 359 completed\n",
      "chunk 360 completed\n",
      "chunk 361 completed\n",
      "chunk 362 completed\n",
      "chunk 363 completed\n",
      "chunk 364 completed\n",
      "chunk 365 completed\n",
      "chunk 366 completed\n",
      "chunk 367 completed\n",
      "chunk 368 completed\n",
      "chunk 369 completed\n",
      "chunk 370 completed\n",
      "chunk 371 completed\n",
      "chunk 372 completed\n",
      "chunk 373 completed\n",
      "chunk 374 completed\n",
      "chunk 375 completed\n",
      "chunk 376 completed\n",
      "chunk 377 completed\n",
      "chunk 378 completed\n",
      "chunk 379 completed\n",
      "chunk 380 completed\n",
      "chunk 381 completed\n",
      "chunk 382 completed\n",
      "chunk 383 completed\n",
      "chunk 384 completed\n",
      "chunk 385 completed\n",
      "chunk 386 completed\n",
      "chunk 387 completed\n",
      "chunk 388 completed\n",
      "chunk 389 completed\n",
      "chunk 390 completed\n",
      "chunk 391 completed\n",
      "chunk 392 completed\n",
      "chunk 393 completed\n",
      "chunk 394 completed\n",
      "chunk 395 completed\n",
      "chunk 396 completed\n",
      "chunk 397 completed\n",
      "chunk 398 completed\n",
      "chunk 399 completed\n",
      "chunk 400 completed\n",
      "chunk 401 completed\n",
      "chunk 402 completed\n",
      "chunk 403 completed\n",
      "chunk 404 completed\n",
      "chunk 405 completed\n",
      "chunk 406 completed\n",
      "chunk 407 completed\n",
      "chunk 408 completed\n",
      "chunk 409 completed\n",
      "chunk 410 completed\n",
      "chunk 411 completed\n",
      "chunk 412 completed\n",
      "chunk 413 completed\n",
      "chunk 414 completed\n",
      "chunk 415 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 416 completed\n",
      "chunk 417 completed\n",
      "chunk 418 completed\n",
      "chunk 419 completed\n",
      "chunk 420 completed\n",
      "chunk 421 completed\n",
      "chunk 422 completed\n",
      "chunk 423 completed\n",
      "chunk 424 completed\n",
      "chunk 425 completed\n",
      "chunk 426 completed\n",
      "chunk 427 completed\n",
      "chunk 428 completed\n",
      "chunk 429 completed\n",
      "chunk 430 completed\n",
      "chunk 431 completed\n",
      "chunk 432 completed\n",
      "chunk 433 completed\n",
      "chunk 434 completed\n",
      "chunk 435 completed\n",
      "chunk 436 completed\n",
      "chunk 437 completed\n",
      "chunk 438 completed\n",
      "chunk 439 completed\n",
      "chunk 440 completed\n",
      "chunk 441 completed\n",
      "chunk 442 completed\n",
      "chunk 443 completed\n",
      "chunk 444 completed\n",
      "chunk 445 completed\n",
      "chunk 446 completed\n",
      "chunk 447 completed\n",
      "chunk 448 completed\n",
      "chunk 449 completed\n",
      "chunk 450 completed\n",
      "chunk 451 completed\n",
      "chunk 452 completed\n",
      "chunk 453 completed\n",
      "chunk 454 completed\n",
      "chunk 455 completed\n",
      "chunk 456 completed\n",
      "chunk 457 completed\n",
      "chunk 458 completed\n",
      "chunk 459 completed\n",
      "chunk 460 completed\n",
      "chunk 461 completed\n",
      "chunk 462 completed\n",
      "chunk 463 completed\n",
      "chunk 464 completed\n",
      "chunk 465 completed\n",
      "chunk 466 completed\n",
      "chunk 467 completed\n",
      "chunk 468 completed\n",
      "chunk 469 completed\n",
      "chunk 470 completed\n",
      "chunk 471 completed\n",
      "chunk 472 completed\n",
      "chunk 473 completed\n",
      "chunk 474 completed\n",
      "chunk 475 completed\n",
      "chunk 476 completed\n",
      "chunk 477 completed\n",
      "chunk 478 completed\n",
      "chunk 479 completed\n",
      "chunk 480 completed\n",
      "chunk 481 completed\n",
      "chunk 482 completed\n",
      "chunk 483 completed\n",
      "chunk 484 completed\n",
      "chunk 485 completed\n",
      "chunk 486 completed\n",
      "chunk 487 completed\n",
      "chunk 488 completed\n",
      "chunk 489 completed\n",
      "chunk 490 completed\n",
      "chunk 491 completed\n",
      "chunk 492 completed\n",
      "chunk 493 completed\n",
      "chunk 494 completed\n",
      "chunk 495 completed\n",
      "chunk 496 completed\n",
      "chunk 497 completed\n",
      "chunk 498 completed\n",
      "chunk 499 completed\n",
      "chunk 500 completed\n",
      "chunk 501 completed\n",
      "chunk 502 completed\n",
      "chunk 503 completed\n",
      "chunk 504 completed\n",
      "chunk 505 completed\n",
      "chunk 506 completed\n",
      "chunk 507 completed\n",
      "chunk 508 completed\n",
      "chunk 509 completed\n",
      "chunk 510 completed\n",
      "chunk 511 completed\n",
      "chunk 512 completed\n",
      "chunk 513 completed\n",
      "chunk 514 completed\n",
      "chunk 515 completed\n",
      "chunk 516 completed\n",
      "chunk 517 completed\n",
      "chunk 518 completed\n",
      "chunk 519 completed\n",
      "chunk 520 completed\n",
      "chunk 521 completed\n",
      "chunk 522 completed\n",
      "chunk 523 completed\n",
      "chunk 524 completed\n",
      "chunk 525 completed\n",
      "chunk 526 completed\n",
      "chunk 527 completed\n",
      "chunk 528 completed\n",
      "chunk 529 completed\n",
      "chunk 530 completed\n",
      "chunk 531 completed\n",
      "chunk 532 completed\n",
      "chunk 533 completed\n",
      "chunk 534 completed\n",
      "chunk 535 completed\n",
      "chunk 536 completed\n",
      "chunk 537 completed\n",
      "chunk 538 completed\n",
      "chunk 539 completed\n",
      "chunk 540 completed\n",
      "chunk 541 completed\n",
      "chunk 542 completed\n",
      "chunk 543 completed\n",
      "chunk 544 completed\n",
      "chunk 545 completed\n",
      "chunk 546 completed\n",
      "chunk 547 completed\n",
      "chunk 548 completed\n",
      "chunk 549 completed\n",
      "chunk 550 completed\n",
      "chunk 551 completed\n",
      "chunk 552 completed\n",
      "chunk 553 completed\n",
      "chunk 554 completed\n",
      "chunk 555 completed\n"
     ]
    }
   ],
   "source": [
    "full_merge = pd.DataFrame()\n",
    "\n",
    "for i in range (0,556):\n",
    "    active_df_small = active_df[10000*i:10000*i+10000]\n",
    "    list_month_merge_small = list_month_merge.loc[list_month_merge['key'].isin(active_df_small['key'])]\n",
    "    partial_merge = list_month_merge_small.merge(active_df_small[['date', 'key']], left_on='key', right_on='key', how='left')\n",
    "    partial_merge = partial_merge.drop(columns=['key', 'id_mo_key'])\n",
    "    full_merge = full_merge.append(partial_merge)\n",
    "    print(\"chunk \" + str(i) + \" completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge = full_merge.drop_duplicates()\n",
    "full_merge = full_merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'date_x', 'book_status', 'seen_avail',\n",
      "       'calendar_price', 'date_notstring', 'id_str', 'mo_yr', 'List_month',\n",
      "       'Listlead1', 'Listlag1', 'headline_price', 'cleaning_fee',\n",
      "       'host_listings_count', 'cum_sum', 'bedrooms', 'room_type',\n",
      "       'neighbourhood', 'zipcode', 'free_park', 'pool', 'host_since',\n",
      "       'review_scores_rating', 'number_of_reviews', 'date_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(full_merge.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge = full_merge.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'res_date', 'booked', 'seen_avail', 'calendar_price', 'late_date',\n",
      "       'id_str', 'mo_yr', 'List_month', 'Listlead1', 'Listlag1',\n",
      "       'headline_price', 'cleaning_fee', 'host_listings_count', 'cum_sum',\n",
      "       'bedrooms', 'room_type', 'neighbourhood', 'zipcode', 'free_park',\n",
      "       'pool', 'host_since', 'review_score', 'number_of_reviews', 'rev_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "full_merge.columns = ['id', 'res_date', 'booked', 'seen_avail', 'calendar_price', 'late_date', 'id_str', 'mo_yr',\n",
    "'List_month', 'Listlead1', 'Listlag1', 'headline_price', 'cleaning_fee', 'host_listings_count',\n",
    "'cum_sum', 'bedrooms', 'room_type', 'neighbourhood', 'zipcode', 'free_park', 'pool', 'host_since',\n",
    "'review_score', 'number_of_reviews','rev_date']\n",
    "\n",
    "print(full_merge.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge['rev_active'] = 1 - full_merge['rev_date'].isna()*1\n",
    "full_merge['composite_active'] = full_merge[['seen_avail', 'rev_active']].values.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id    res_date  booked  seen_avail  calendar_price  late_date  id_str  \\\n",
      "0   53940  2015-09-02       0           1            77.0 2015-09-02   53940   \n",
      "1   53940  2015-09-06       0           1            86.0 2015-09-06   53940   \n",
      "2   53940  2015-09-07       0           1            80.0 2015-09-07   53940   \n",
      "3   53940  2015-09-08       0           1            75.0 2015-09-08   53940   \n",
      "4   53940  2015-09-09       0           1            75.0 2015-09-09   53940   \n",
      "5  115681  2015-09-12       0           1            88.0 2015-09-12  115681   \n",
      "6   53940  2015-09-13       0           1            75.0 2015-09-13   53940   \n",
      "7   53940  2015-09-14       0           1            75.0 2015-09-14   53940   \n",
      "8   53940  2015-09-15       0           1            75.0 2015-09-15   53940   \n",
      "9   53940  2015-09-16       0           1            75.0 2015-09-16   53940   \n",
      "\n",
      "     mo_yr  List_month  Listlead1  ...  neighbourhood  zipcode  free_park  \\\n",
      "0  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "1  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "2  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "3  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "4  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "5  2015-09         1.0        1.0  ...       Piedmont    97217        0.0   \n",
      "6  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "7  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "8  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "9  2015-09         1.0        1.0  ...          Kerns    97232        0.0   \n",
      "\n",
      "   pool           host_since  review_score number_of_reviews   rev_date  \\\n",
      "0   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-02   \n",
      "1   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-06   \n",
      "2   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-07   \n",
      "3   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-08   \n",
      "4   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-09   \n",
      "5   0.0  2011-05-13 00:00:00          99.0             147.0 2015-09-12   \n",
      "6   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-13   \n",
      "7   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-14   \n",
      "8   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-15   \n",
      "9   0.0  2010-08-19 00:00:00         100.0               7.0 2015-09-16   \n",
      "\n",
      "  rev_active  composite_active  \n",
      "0          1                 1  \n",
      "1          1                 1  \n",
      "2          1                 1  \n",
      "3          1                 1  \n",
      "4          1                 1  \n",
      "5          1                 1  \n",
      "6          1                 1  \n",
      "7          1                 1  \n",
      "8          1                 1  \n",
      "9          1                 1  \n",
      "\n",
      "[10 rows x 27 columns]\n",
      "(1719080, 27)\n"
     ]
    }
   ],
   "source": [
    "full_merge_tosave = full_merge.copy()\n",
    "full_merge_tosave = full_merge_tosave.drop_duplicates()\n",
    "\n",
    "print(full_merge_tosave.head(10))\n",
    "print(full_merge.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge_tosave.to_csv(\"Portland_cal_rev_list_FULLMERGEv4_10_days_before.csv.gz\", compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
