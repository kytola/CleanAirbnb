{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar Integration partB\n",
    "\n",
    "This Jupyter notebook contains the second half of calendar integration and it produces a dataframe named \"cal_rev_list_FULLMERGEv3_10_days_before.csv\" which is then saved in the Saved Data folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Universal Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable garbage collection module for memory purposes\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/'\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many days before a review a property is assumed to be active. Important assumption!\n",
    "N_days_before = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-01-04', '2018-11-07', '2015-05-12', '2016-02-03', '2016-09-04', '2017-04-07', '2017-10-04', '2018-02-08', '2018-04-11', '2016-08-04', '2018-07-10', '2016-05-03', '2017-05-07', '2015-09-02', '2018-01-16', '2018-10-09', '2015-03-01', '2019-01-13', '2017-12-09', '2017-02-09', '2016-04-05', '2017-08-06', '2016-06-03']\n"
     ]
    }
   ],
   "source": [
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)      \n",
    "\n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "init_dates = file_dates[3:26]\n",
    "\n",
    "print(init_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload listings dataframe/cross sectional file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natfern/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_path)\n",
    "\n",
    "listings_df = pd.read_csv('2ndStageClean_Portland.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add free parking and pool amenities\n",
    "parking_search = ['Free parking on premises']\n",
    "listings_df.loc[:, 'free_park'] = (listings_df['amenities'].str.contains('|'.join(parking_search), na=False)*1).values\n",
    "\n",
    "pool_search = ['pool', 'Pool']\n",
    "listings_df.loc[:, 'pool'] = (listings_df['amenities'].str.contains('|'.join(pool_search), na=False)*1).values\n",
    "\n",
    "os.chdir(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the booked_df that is created by previous Jupyter Notebook\n",
    "\n",
    "os.chdir(data_path)\n",
    "booked_df = pd.read_csv('rejoined_booked_df.csv')\n",
    "booked_df['date_notstring'] = booked_df['date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating review files\n",
    "\n",
    "If a property receives a review then I assume that it was available for the a certain number of days prior to that review.\n",
    "\n",
    "The commented out code in this section creates the review dataframe as long as you have all of the review scrapes in the \"csv_path\" folder. After it has been created, all that needs to be done is to load it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Collect review csv names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect reviews csvs names\n",
    "\n",
    "os.chdir(csv_path)\n",
    "revFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"reviews.csv.gz\"):\n",
    "        revFiles.append(fileNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create a function to concatenate review dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(START, END):\n",
    "    sheets_df = []\n",
    "    \n",
    "    for filename in revFiles[START:END]:\n",
    "        df = pd.read_csv(filename, index_col = None, header=0)\n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Run the function and save a compressed dataframe with all unique reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6856948 494076\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE, THIS CREATES THE AGGREGATE REVIEW FILE\n",
    "\n",
    "collected_revs = concat_spreadsheets(3, len(revFiles))\n",
    "uniq_revs = collected_revs.drop_duplicates()\n",
    "print(len(collected_revs), len(uniq_revs))\n",
    "uniq_revs.to_csv('Revs_portland.csv.gz', compression='gzip') \n",
    "#This creates the compressed revdate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Revs_portland.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4715cd6db271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0muniq_revs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Revs_portland.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 handle = gzip.GzipFile(\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Revs_portland.csv.gz'"
     ]
    }
   ],
   "source": [
    "# This cell imports the compressed review datafame collected above. Update the filename \n",
    "# accordingly when additional dates are added. \n",
    "\n",
    "os.chdir(data_path)\n",
    "\n",
    "uniq_revs = pd.read_csv('Revs_portland.csv.gz', compression='gzip')\n",
    "\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the listing IDs that are not in the calendar files\n",
    "unique_ids = booked_df.id.unique()\n",
    "uniq_revs = uniq_revs[uniq_revs.listing_id.isin(unique_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natfern/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>12899</td>\n",
       "      <td>2015-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>12899</td>\n",
       "      <td>2015-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>12899</td>\n",
       "      <td>2015-11-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>12899</td>\n",
       "      <td>2015-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>12899</td>\n",
       "      <td>2015-11-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     listing_id       date\n",
       "320       12899 2015-11-04\n",
       "321       12899 2015-11-08\n",
       "322       12899 2015-11-10\n",
       "323       12899 2015-11-14\n",
       "324       12899 2015-11-18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_min, date_max = booked_df['date'].min(), booked_df['date'].max() # Use the dates in my calendar data files\n",
    "\n",
    "revs_df = uniq_revs[['listing_id', 'date']]\n",
    "\n",
    "revs_df.loc[:, 'date'] = revs_df['date'].astype('datetime64')\n",
    "mask = (revs_df['date'] >= date_min) & (revs_df['date'] <= date_max)\n",
    "revs_df = revs_df[mask]\n",
    "revs_short = revs_df \n",
    "\n",
    "# Dataframe of dates\n",
    "reviews_file = revs_short\n",
    "N_reviews = len(reviews_file)\n",
    "reviews_file.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active property dataframe creation\n",
    "\n",
    "Much like the review code, the commented out block below creates a pandas dataframe that tracks the dates during which a property is active.\n",
    "\n",
    "Once the dataframe is created it can just be loaded in, meaning you can comment out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139669\n"
     ]
    }
   ],
   "source": [
    "# Here I identify the unique dates from the reviews_file and identify the \n",
    "# index for every row of the reviews_file that maps to the unique dates file. \n",
    "# This allows me to avoid duplicate calculations for identical dates.\n",
    "\n",
    "\n",
    "my_dates = reviews_file['date'].astype('datetime64[D]')\n",
    "\n",
    "unq_dates = pd.DataFrame([my_dates.unique()]).T\n",
    "unq_dates.columns = ['unq_dates']\n",
    "unq_dates = unq_dates.sort_values(by='unq_dates').reset_index(drop=True)\n",
    "\n",
    "# # Identify the valid index in the unq_arr for the non-unique list of dates\n",
    "\n",
    "dates_arr = my_dates.values\n",
    "unq_arr = np.concatenate(unq_dates.values)\n",
    "\n",
    "sorter = np.argsort(unq_arr)\n",
    "my_indices = sorter[np.searchsorted(unq_arr, dates_arr, sorter=sorter)]\n",
    "\n",
    "my_dates = pd.DataFrame(my_dates.values, columns = ['dates'])\n",
    "my_dates.loc[:, \"unq_ind\"] = my_indices\n",
    "unq_dates.loc[np.r_[my_dates['unq_ind']], :] # Don't delete! This lets me explore what is going on\n",
    "\n",
    "# # Here I create all of the date ranges that I want to work with.\n",
    "\n",
    "unq_dates.loc[:,\"start\"] = unq_dates['unq_dates'] - datetime.timedelta(days=N_days_before)\n",
    "\n",
    "days_before = []\n",
    "\n",
    "for i in range(len(unq_dates)):\n",
    "     days_before.append(pd.date_range(unq_dates['start'][i], unq_dates['unq_dates'][i], closed='left'))\n",
    "    \n",
    " #------ Here I create a dataframe that has all of the listing, takes at least 4.5 minutes ------------------------------\n",
    "\n",
    "active_dates = np.concatenate(np.array(days_before)[my_dates['unq_ind'].values].astype('datetime64[D]'))\n",
    "my_date_ids = np.reshape(pd.concat([reviews_file['listing_id']]*N_days_before, axis=1).values, (N_days_before*N_reviews,1))\n",
    "my_date_ids = np.concatenate(my_date_ids)\n",
    "\n",
    "active_df = pd.DataFrame([my_date_ids, active_dates]).T\n",
    "active_df.columns = ['id', 'date']\n",
    "active_df.loc[:, \"key\"] = active_df['id'].astype(str) + \":\" + active_df['date'].astype(str)\n",
    "\n",
    "print(N_reviews)\n",
    "active_df.head(5)\n",
    "\n",
    "active_df.to_csv('active_dates'+ str(N_days_before) + \".csv.gz\", compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_active_file is created by the code that is commented out below! \n",
    "\n",
    "my_active_file = \"active_dates\" + str(N_days_before) + \".csv.gz\"\n",
    "\n",
    "active_df = pd.read_csv(my_active_file, compression='gzip', parse_dates=['date'])\n",
    "active_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-merge processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df['id_str']= booked_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df.loc[:, \"key\"] = booked_df['id_str'] + \":\" + booked_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df.loc[:, 'mo_yr'] = booked_df['date_notstring'].dt.to_period('M')\n",
    "booked_df.loc[:, \"id_mo_key\"] = booked_df['mo_yr'].astype(str) + \":\" + booked_df['id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df['batch_YRMO'] = listings_df['batch_YRMO'].astype('datetime64').dt.to_period('M')\n",
    "listings_mask = (listings_df['batch_YRMO'] >= booked_df['mo_yr'].min()) & (listings_df['batch_YRMO'] <= booked_df['mo_yr'].max())\n",
    "listings_df = listings_df[listings_mask]\n",
    "listings_df.loc[:, \"id_mo_key\"] = listings_df['batch_YRMO'].astype(str) + \":\" + listings_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listings_df.columns[0:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge time-varying variables from the panel to the booked_df dataframe. Prices here are \"headline prices\".\n",
    "\n",
    "list_month_merge = booked_df.merge(listings_df[['id_mo_key', 'List_month', 'Listlead1', \n",
    "                                                'Listlag1', 'price', 'cleaning_fee', 'host_listings_count', 'cum_sum',\n",
    "                                                 'bedrooms', 'room_type', 'neighbourhood', 'zipcode', # I wanted to have this last row separte \n",
    "                                               'free_park', 'pool', 'host_since']], # These two are new here\n",
    "                                   left_on='id_mo_key', right_on='id_mo_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del booked_df\n",
    "del listings_df\n",
    "del revs_df\n",
    "del listings_mask\n",
    "del revs_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1396690"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 completed\n",
      "chunk 1 completed\n",
      "chunk 2 completed\n",
      "chunk 3 completed\n",
      "chunk 4 completed\n",
      "chunk 5 completed\n",
      "chunk 6 completed\n",
      "chunk 7 completed\n",
      "chunk 8 completed\n",
      "chunk 9 completed\n",
      "chunk 10 completed\n",
      "chunk 11 completed\n",
      "chunk 12 completed\n",
      "chunk 13 completed\n",
      "chunk 14 completed\n",
      "chunk 15 completed\n",
      "chunk 16 completed\n",
      "chunk 17 completed\n",
      "chunk 18 completed\n",
      "chunk 19 completed\n",
      "chunk 20 completed\n",
      "chunk 21 completed\n",
      "chunk 22 completed\n",
      "chunk 23 completed\n",
      "chunk 24 completed\n",
      "chunk 25 completed\n",
      "chunk 26 completed\n",
      "chunk 27 completed\n",
      "chunk 28 completed\n",
      "chunk 29 completed\n",
      "chunk 30 completed\n",
      "chunk 31 completed\n",
      "chunk 32 completed\n",
      "chunk 33 completed\n",
      "chunk 34 completed\n",
      "chunk 35 completed\n",
      "chunk 36 completed\n",
      "chunk 37 completed\n",
      "chunk 38 completed\n",
      "chunk 39 completed\n",
      "chunk 40 completed\n",
      "chunk 41 completed\n",
      "chunk 42 completed\n",
      "chunk 43 completed\n",
      "chunk 44 completed\n",
      "chunk 45 completed\n",
      "chunk 46 completed\n",
      "chunk 47 completed\n",
      "chunk 48 completed\n",
      "chunk 49 completed\n",
      "chunk 50 completed\n",
      "chunk 51 completed\n",
      "chunk 52 completed\n",
      "chunk 53 completed\n",
      "chunk 54 completed\n",
      "chunk 55 completed\n",
      "chunk 56 completed\n",
      "chunk 57 completed\n",
      "chunk 58 completed\n",
      "chunk 59 completed\n",
      "chunk 60 completed\n",
      "chunk 61 completed\n",
      "chunk 62 completed\n",
      "chunk 63 completed\n",
      "chunk 64 completed\n",
      "chunk 65 completed\n",
      "chunk 66 completed\n",
      "chunk 67 completed\n",
      "chunk 68 completed\n",
      "chunk 69 completed\n",
      "chunk 70 completed\n",
      "chunk 71 completed\n",
      "chunk 72 completed\n",
      "chunk 73 completed\n",
      "chunk 74 completed\n",
      "chunk 75 completed\n",
      "chunk 76 completed\n",
      "chunk 77 completed\n",
      "chunk 78 completed\n",
      "chunk 79 completed\n",
      "chunk 80 completed\n",
      "chunk 81 completed\n",
      "chunk 82 completed\n",
      "chunk 83 completed\n",
      "chunk 84 completed\n",
      "chunk 85 completed\n",
      "chunk 86 completed\n",
      "chunk 87 completed\n",
      "chunk 88 completed\n",
      "chunk 89 completed\n",
      "chunk 90 completed\n",
      "chunk 91 completed\n",
      "chunk 92 completed\n",
      "chunk 93 completed\n",
      "chunk 94 completed\n",
      "chunk 95 completed\n",
      "chunk 96 completed\n",
      "chunk 97 completed\n",
      "chunk 98 completed\n",
      "chunk 99 completed\n",
      "chunk 100 completed\n",
      "chunk 101 completed\n",
      "chunk 102 completed\n",
      "chunk 103 completed\n",
      "chunk 104 completed\n",
      "chunk 105 completed\n",
      "chunk 106 completed\n",
      "chunk 107 completed\n",
      "chunk 108 completed\n",
      "chunk 109 completed\n",
      "chunk 110 completed\n",
      "chunk 111 completed\n",
      "chunk 112 completed\n",
      "chunk 113 completed\n",
      "chunk 114 completed\n",
      "chunk 115 completed\n",
      "chunk 116 completed\n",
      "chunk 117 completed\n",
      "chunk 118 completed\n",
      "chunk 119 completed\n",
      "chunk 120 completed\n",
      "chunk 121 completed\n",
      "chunk 122 completed\n",
      "chunk 123 completed\n",
      "chunk 124 completed\n",
      "chunk 125 completed\n",
      "chunk 126 completed\n",
      "chunk 127 completed\n",
      "chunk 128 completed\n",
      "chunk 129 completed\n",
      "chunk 130 completed\n",
      "chunk 131 completed\n",
      "chunk 132 completed\n",
      "chunk 133 completed\n",
      "chunk 134 completed\n",
      "chunk 135 completed\n",
      "chunk 136 completed\n",
      "chunk 137 completed\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,138):\n",
    "    active_df_small = active_df[10000*i:10000*i+10000]\n",
    "    list_month_merge_small = list_month_merge.loc[list_month_merge['key'].isin(active_df_small['key'])]\n",
    "    partial_merge = list_month_merge_small.merge(active_df_small[['date', 'key']], left_on='key', right_on='key', how='left')\n",
    "    partial_merge = partial_merge.drop(columns=['key', 'id_mo_key'])\n",
    "    full_merge = full_merge.append(partial_merge)\n",
    "    print(\"chunk \" + str(i) + \" completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge = full_merge.drop_duplicates()\n",
    "full_merge = full_merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge.columns = [ 'Unnamed: 0','id', 'res_date', 'booked', 'seen_avail', 'cal_price_booking', 'last_date', 'id_str',\n",
    "       'mo_yr', 'List_month', 'Listlead1', 'Listlag1', 'cal_final_prices', 'cleaning_fee', 'host_listings_count',\n",
    "                      'cum_sum', 'bedrooms', 'room_type', 'neighbourhood', 'zipcode', 'free_park', 'pool', 'host_since','rev_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge['rev_active'] = 1 - full_merge['rev_date'].isna()*1\n",
    "full_merge['composite_active'] = full_merge[['seen_avail', 'rev_active']].values.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge_tosave = full_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge_tosave = full_merge_tosave.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merge_tosave.to_csv(\"cal_rev_list_FULLMERGEv3_10_days_before.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
