{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Stage Panel Data Cleaning\n",
    "This second data cleaning notebook cleans the output csv of the first round of data cleaning by applying less economically non-controversial cleaning methods. It also provides some visualizations of the cleaned data thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import and Setup Universal Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import operator \n",
    "import datetime\n",
    "import quantecon as qe\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Make sure repository has a 2. Clean data and Saved data folders!\n",
    "csv_save_path = cwd2 + '/Saved data'\n",
    "# Revert to preliminary directory\n",
    "os.chdir(csv_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary formatting\n",
    "dateCols =['last_scraped', 'host_since', 'first_review', 'last_review', 'scrape_batch']\n",
    "df = pd.read_csv('1stStageClean_SAN.csv', low_memory=False, parse_dates=dateCols)\n",
    "\n",
    "df = df[df.columns.drop(list(df.filter(regex='NORl')))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 1st Stage Cleaned Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.groupby('scrape_batch')['List_month'].sum().index\n",
    "counts = df.groupby('scrape_batch')['List_month'].sum().values\n",
    "\n",
    "count_max = counts.max()\n",
    "count_min = counts.min()\n",
    "count_std = counts.std()\n",
    "\n",
    "plt.plot(dates, counts)\n",
    "plt.ylim(0, count_max + 300)\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()\n",
    "\n",
    "print(round(count_max/count_min - 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagsleads(var, lag_range, df, title):\n",
    "    \"\"\"\n",
    "    This function creates lag and lead variables for a given variable and dataframe.    \n",
    "    \"\"\"\n",
    "    df = df.sort_values(by = ['id', 'month'])\n",
    "    \n",
    "    for i in range(-lag_range, lag_range + 1):\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        if i < 0:\n",
    "            df.loc[:, title + \"lead\" + str(abs(i)) ] = df.groupby('id')[var].shift(i)\n",
    "                \n",
    "        if i > 0: \n",
    "            df.loc[:, title + \"lag\" + str(abs(i)) ] = df.groupby('id')[var].shift(i)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confidence_cutoff = 1.960 \n",
    "\n",
    "df.loc[:, 'corrected_NOR'] = df.groupby(['id'])['number_of_reviews'].rolling(19, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "df = create_lagsleads('corrected_NOR', 12, df, \"NOR\") \n",
    "df.loc[:, \"NOR_diff\"] = df['corrected_NOR'] - df['NORlag1']\n",
    "\n",
    "# This bound is arbitrary \n",
    "bounds = df.groupby('id')['NOR_diff'].mean() + Confidence_cutoff*df.groupby('id')['NOR_diff'].std()/np.sqrt(df.groupby('id')['List_month'].sum())\n",
    "df.loc[:, \"NOR_diff_bound\"] = bounds[df['id']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timedelta_formatter(var):\n",
    "    \"\"\"\n",
    "    This function formats the time delta for a passed variable.\n",
    "    \"\"\"\n",
    "    df.loc[:, var] = pd.to_timedelta(df[var]).dt.days\n",
    "    \n",
    "for deltas in ['days_since_rev', 'days_since_first_rev', 'host_length']:\n",
    "    timedelta_formatter(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_var_change(var, cutoff, relate, df):\n",
    "    \"\"\"\n",
    "    This function produces ids where the variable of interest changes.\n",
    "    \"\"\"\n",
    "    ops = {'>': operator.gt,\n",
    "       '<': operator.lt,\n",
    "       '>=': operator.ge,\n",
    "       '<=': operator.le,\n",
    "       '==': operator.eq}\n",
    "    \n",
    "    # Take ids and variable of interest and drop any na's\n",
    "    repetition_arr = np.array(df[['id', var]].dropna().drop_duplicates()) # Need drop_duplicates to identify actual price changes\n",
    "    counts = np.unique(repetition_arr[:,0], return_counts = True)\n",
    "    return counts[0][ops[relate](counts[1], cutoff)], counts[1][ops[relate](counts[1], cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_ids, change_counts = test_var_change('property_type', 2, '>=', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the location of the first month hosted\n",
    "first_host_ind = df.groupby('id').List_month.idxmax()\n",
    "df.loc[:, \"first_appearance\"] = (df.index == first_host_ind[df['id']]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the location of the last month hosted\n",
    "df_list = df[df['List_month'] == 1]\n",
    "last = df_list.groupby('id')['month'].last()\n",
    "\n",
    "df.loc[:, 'last_app'] = (df['month'].values == last[df['id']].values).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of when new properties show up in the data\n",
    "plt.plot(df.groupby('scrape_batch')['last_app'].sum()[1:-1], label='Sum last appearance')\n",
    "plt.plot(df.groupby('scrape_batch')['first_appearance'].sum()[1:-1], label='Sum first appearances')\n",
    "plt.title('Count of first and last appearances of Airbnbs on platform in San Francisco (excluding first and last month, raw data)')\n",
    "plt.ylim(90,1200)\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative listings for the host\n",
    "host_cumlists = df.groupby(['host_id', 'month'])['first_appearance'].sum().unstack().cumsum(axis=1).stack().astype(int)\n",
    "host_cumlists.name = 'cum_sum'\n",
    "df = df.join(host_cumlists, on=['host_id', 'month'], rsuffix='_cumsum')\n",
    "\n",
    "# Calculate some other summary statistics about host holdings\n",
    "df = df.join(df.groupby(['host_id', 'month'])['List_month'].sum(), on=['host_id', 'month'], rsuffix='_byhost_month')\n",
    "df = df.join(df.groupby(['host_id'])['List_month'].sum(), on=['host_id'], rsuffix='_host_overall')\n",
    "df = df.join(df.groupby(['id'])['List_month'].sum(), on=['id'], rsuffix='_id_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hotels in my data\n",
    "df.loc[:, 'hotel_dum'] = np.array((df['property_type'] == \"Boutique hotel\") | (df['property_type'] == \"Bed and breakfast\") | (df['property_type'] == \"Boutique hotel\") | (df['property_type'] == \"Aparthotel\")| (df['property_type'] == \"Hotel\")| (df['property_type'] == \"Resort\")| (df['property_type'] == \"Serviced apartment\") )*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.groupby('scrape_batch')['hotel_dum'].sum()[1:])\n",
    "plt.title(\"Count of hotels on Airbnb in San Francisco over time (raw data)\")\n",
    "plt.ylim(0, 1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,\"entrant\"] =  np.array((df['first_appearance'] == 1) & (df['days_since_first_rev'] < 30 ) & (df['number_of_reviews'] < 10 ))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.groupby('scrape_batch')['entrant'].sum()[1:])\n",
    "plt.title(\"Count of 'entrants' onto San Francisco Airbnb over time (raw data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.groupby(['neighbourhood', 'month'])['List_month'].sum(), on=['neighbourhood', 'month'], rsuffix='_byneigh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.sort_values(by=['neighbourhood', 'month']).groupby(['neighbourhood', 'month'])['List_month'].sum().shift(1), on=['neighbourhood', 'month'], rsuffix='_lag_byneigh')\n",
    "df.loc[:,'List_month_lag_byneigh'] =  df['List_month_lag_byneigh'].mask(df['month'] == 3, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['neighbourhood', 'month'])[['id', 'month', 'neighbourhood', 'List_month_byneigh', 'List_month_lag_byneigh']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Drop Indicators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'drop_indicator'] = 0\n",
    "\n",
    "max_NORdiff = df.groupby('id')['NOR_diff'].max()\n",
    "df.loc[:, 'max_NORdiff'] = max_NORdiff[df['id']].values\n",
    "\n",
    "no_revs_ind = (df['max_NORdiff'] == 0).values*1\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: no_revs_ind})\n",
    "\n",
    "df['drop_indicator'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely low or high prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_1per = df.price.quantile(.01)\n",
    "price_99per = df.price.quantile(.99)\n",
    "\n",
    "low_price = (df.groupby('id')['price'].min()[df['id']].values < price_1per)*1\n",
    "high_price = (df.groupby('id')['price'].min()[df['id']].values > price_99per)*1\n",
    "\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: low_price})\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: high_price})\n",
    "\n",
    "df['drop_indicator'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Never state a day of availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_avail = (df.groupby('id')['availability_365'].max()[df['id']].values == 0)*1\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: never_avail})\n",
    "\n",
    "df['drop_indicator'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_term_rental = (df.groupby(['id'])['minimum_nights'].min()[df['id']].values > 30)*1\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: long_term_rental})\n",
    "df['drop_indicator'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_ind = (df.groupby(['id'])['hotel_dum'].max()[df['id']].values == 1)*1\n",
    "df.loc[:, 'drop_indicator'] = df['drop_indicator'].replace({ 0: hotel_ind})\n",
    "\n",
    "df['drop_indicator'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['drop_indicator'] == 1]))\n",
    "print(len(df[df['drop_indicator'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finalize and Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('2ndStageClean_SAN.csv', index=False, date_format='%Y-%m-%d %H:%M:%S')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
