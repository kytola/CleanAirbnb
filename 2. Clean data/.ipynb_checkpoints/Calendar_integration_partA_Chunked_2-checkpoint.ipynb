{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar_integration_partA\n",
    "\n",
    "## Created June 19, 2020\n",
    "\n",
    "This file is the first half of the file \"CALENDAR DATA_05\", and it produces a dataframe named 'rejoined_booked_df.csv.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import time\n",
    "from pandas.core.common import SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/'\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-09-14', '2016-09-04', '2017-06-05', '2017-03-05', '2018-01-16', '2017-11-13', '2018-04-11', '2016-04-05', '2017-09-12', '2017-10-04', '2016-11-06', '2018-12-10', '2017-07-06', '2018-05-13', '2016-08-04', '2018-02-08', '2017-01-04', '2017-08-06', '2016-01-01', '2018-11-07', '2019-01-13', '2015-12-02', '2016-12-08', '2015-11-02', '2017-05-07', '2019-02-06', '2016-05-03', '2017-12-09', '2016-02-03', '2018-10-09', '2018-08-14', '2018-07-10', '2016-06-03', '2017-04-07', '2016-07-04', '2017-02-09', '2015-09-02']\n"
     ]
    }
   ],
   "source": [
    "# This loop aggregates all calendar file names\n",
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)\n",
    "        \n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "init_dates = file_dates\n",
    "del init_dates[9]\n",
    "del init_dates[16]\n",
    "print(init_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read 2nd stage cleaned dataframe\n",
    "os.chdir(data_path)\n",
    "\n",
    "dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "listings_df = pd.read_csv('2ndStageClean_Portland.csv.gz', compression = 'gzip', low_memory=False, parse_dates=dateCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def complex_extract_cal_list(date, cal_start, cal_end, update_cutoff=90, avail_cutoff=0):\n",
    "    warnings.simplefilter(\"ignore\", category=SettingWithCopyWarning)\n",
    "    \"\"\"\n",
    "    The point of this function is to remove listings that seem dormant \n",
    "    on Airbnb from the calendar files that we will be using.\n",
    "    \"\"\"\n",
    "    # Managing listings dataframe \n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv\", low_memory = False, parse_dates = dateCols)\n",
    "    \n",
    "    # Here I try to identify which listing calendars should be trusted, I try not to be too harsh.\n",
    "    small = list_file['calendar_updated'].str.split(' ', 3, expand=True)\n",
    "    small.columns = ['count', 'measure', 'length']\n",
    "    small = small[['count', 'measure']]\n",
    "    small = small.replace([\"days\", 'week', 'weeks', 'months', 'today', 'never','yesterday'], \n",
    "                          [1, 7, 7, 30, 0, 999, 1])\n",
    "    small['count'] = small['count'].replace('a', 1)\n",
    "    small = small.fillna(1)\n",
    "\n",
    "    list_file.loc[:, 'update_numeric'] = small['count'].astype(float)*small['measure'].astype(float)\n",
    "\n",
    "    checks = list_file[['update_numeric', 'instant_bookable', 'last_scraped', 'last_review', 'availability_365']]\n",
    "    checks.loc[:, 'DSR'] = (checks['last_scraped'] - checks['last_review']).dt.days\n",
    "    checks = checks[['update_numeric', 'instant_bookable', 'DSR', 'availability_365']]\n",
    "    \n",
    "\n",
    "    list_file.loc[:, 'DSR'] = checks['DSR']\n",
    "    list_file.loc[:, 'cal_trust'] = (((checks['update_numeric'] <= update_cutoff) | (checks['instant_bookable'] =='t') \n",
    "                                     | (checks['availability_365'] > avail_cutoff))*1) #      \n",
    "    trusted_ids = list_file[list_file['cal_trust'] == 1]['id'].unique()\n",
    "    \n",
    "    list_file['price'] = list_file.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "     # Reduce the size of variables in the listings dataframe.\n",
    "    list_file_small = list_file[['id', 'price', 'host_listings_count','bedrooms', 'bathrooms', 'neighbourhood', 'zipcode', 'room_type', \n",
    "                                 'instant_bookable', 'update_numeric', 'DSR', 'cal_trust', 'availability_365']]\n",
    "   \n",
    "    list_file_small.columns = ['id', 'headline_price', 'host_lists', 'bedrooms', \n",
    "                               'bathrooms', 'neigh', 'zip', 'type', 'instant', \n",
    "                               'DSupdate', 'DSReview', 'Active_flag', 'avail365']\n",
    "\n",
    "    # Managing calendar dataframe\n",
    "    cal_file = pd.read_csv(\"united-states_portland_\" + date + \"_calendar.csv.gz\", compression = 'gzip')\n",
    "    cal_file = cal_file[['listing_id', 'date','available', 'price']] # This is needed because newer calendar files add cols\n",
    "    cal_file = cal_file[(cal_file['date'] >= cal_start)& (cal_file['date'] <= cal_end)] # Deal with different scrape times\n",
    "    cal_file['price'] = cal_file.price.replace('[\\$,]', '', regex=True).astype(float)   # Prices converted to floats\n",
    "    \n",
    "    print(\"----Cleaning calendar and listings data for \" + date + \"----\")\n",
    "    print(\"Listing-dates removed: \" + str(float(len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))))\n",
    "    print(\"Unique listings removed: \" + str(float((len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))/365)))\n",
    "    print(\"                                                           \")\n",
    "    \n",
    "    cal_file = cal_file[cal_file['listing_id'].isin(trusted_ids)] # Removes questionable calendar data\n",
    "   \n",
    "    # Rename columns\n",
    "    cal_file.columns = ['id', 'date', 'avail', 'night_price']\n",
    "    \n",
    "    return cal_file, list_file_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listings_df.loc[:, 'List_month'] = listings_df['List_month'] - listings_df['Occasional_LTR'] \n",
    "\n",
    "#listings_df = listings_df[listings_df['Occasional_LTR'] == 0] # This may mess up some of the dimensionsl\n",
    "\n",
    "os.chdir(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data cleaning and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-09-02' '2015-11-02' '2015-12-02' '2016-01-01' '2016-02-03'\n",
      " '2016-04-05' '2016-05-03' '2016-06-03' '2016-07-04' '2016-08-04'\n",
      " '2016-09-04' '2016-11-06' '2016-12-08' '2017-01-04' '2017-02-09'\n",
      " '2017-03-05' '2017-04-07' '2017-05-07' '2017-06-05' '2017-07-06'\n",
      " '2017-08-06' '2017-09-12' '2017-10-04' '2017-11-13' '2017-12-09'\n",
      " '2018-01-16' '2018-02-08' '2018-04-11' '2018-05-13' '2018-07-10'\n",
      " '2018-08-14' '2018-09-14' '2018-10-09' '2018-11-07' '2018-12-10'\n",
      " '2019-01-13' '2019-02-06']\n"
     ]
    }
   ],
   "source": [
    "#This section focuses on date formatting and array creation:\n",
    "arr_dates_init = np.array(init_dates).astype('datetime64')\n",
    "arr_dates_init.sort()\n",
    "arr_dates = arr_dates_init\n",
    "print(arr_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dates_2 = list((arr_dates).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No issue\n",
      "1\n",
      "No issue\n",
      "2\n",
      "No issue\n",
      "3\n",
      "No issue\n",
      "4\n",
      "No issue\n",
      "5\n",
      "No issue\n",
      "6\n",
      "No issue\n",
      "7\n",
      "No issue\n",
      "8\n",
      "No issue\n",
      "9\n",
      "No issue\n",
      "10\n",
      "No issue\n",
      "11\n",
      "No issue\n",
      "12\n",
      "No issue\n",
      "13\n",
      "No issue\n",
      "14\n",
      "No issue\n",
      "15\n",
      "Issue!\n",
      "['2017-03-06T00:00:00.000000000' '2017-03-05T00:00:00.000000000']\n",
      "16\n",
      "Issue!\n",
      "['2017-04-07T00:00:00.000000000' '2017-04-08T00:00:00.000000000']\n",
      "17\n",
      "No issue\n",
      "18\n",
      "No issue\n",
      "19\n",
      "No issue\n",
      "20\n",
      "No issue\n",
      "21\n",
      "No issue\n",
      "22\n",
      "No issue\n",
      "23\n",
      "No issue\n",
      "24\n",
      "No issue\n",
      "25\n",
      "Issue!\n",
      "['2018-01-16T00:00:00.000000000' '2018-01-17T00:00:00.000000000']\n",
      "26\n",
      "No issue\n",
      "27\n",
      "Issue!\n",
      "['2018-04-11T00:00:00.000000000' '2018-04-12T00:00:00.000000000']\n",
      "28\n",
      "Issue!\n",
      "['2018-05-14T00:00:00.000000000' '2018-05-13T00:00:00.000000000']\n",
      "29\n",
      "No issue\n",
      "30\n",
      "No issue\n",
      "31\n",
      "No issue\n",
      "32\n",
      "Issue!\n",
      "['2018-10-09T00:00:00.000000000' '2018-10-12T00:00:00.000000000']\n",
      "33\n",
      "Issue!\n",
      "['2018-11-07T00:00:00.000000000' '2018-11-09T00:00:00.000000000']\n",
      "34\n",
      "No issue\n",
      "35\n",
      "No issue\n",
      "36\n",
      "No issue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loop simply identifies when a scrape took more than one day, \n",
    "# and creates an adjustment layer based on this information\n",
    "os.chdir(csv_path)\n",
    "i = 0\n",
    "adjustment_layer = []\n",
    "for date in init_dates_2:\n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv\", low_memory = False, parse_dates = dateCols)\n",
    "    print(i)\n",
    "    if len(list_file.last_scraped.unique()) > 1:\n",
    "        print(\"Issue!\")\n",
    "        print(list_file.last_scraped.unique())\n",
    "        adjustment_layer.append(len(list_file.last_scraped.unique()))\n",
    "    else:\n",
    "        print(\"No issue\")\n",
    "        adjustment_layer.append(0)\n",
    "    i += 1\n",
    "adjustment_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Cleaning calendar and listings data for 2015-09-02----\n",
      "Listing-dates removed: 730.0\n",
      "Unique listings removed: 2.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-11-02----\n",
      "Listing-dates removed: 1825.0\n",
      "Unique listings removed: 5.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-12-02----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-01-01----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-02-03----\n",
      "Listing-dates removed: 25116.0\n",
      "Unique listings removed: 68.81095890410958\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-04-05----\n",
      "Listing-dates removed: 41610.0\n",
      "Unique listings removed: 114.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-05-03----\n",
      "Listing-dates removed: 45990.0\n",
      "Unique listings removed: 126.0\n",
      "                                                           \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113593/46576276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmy_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlisting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         cal, listing = complex_extract_cal_list(str(arr_dates[i]), str(arr_dates[i] + adjustment_layer[i]), str(arr_dates[i] + \\\n\u001b[0m\u001b[1;32m     11\u001b[0m         (364)))\n\u001b[1;32m     12\u001b[0m         \u001b[0mmy_cals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_113593/1879212348.py\u001b[0m in \u001b[0;36mcomplex_extract_cal_list\u001b[0;34m(date, cal_start, cal_end, update_cutoff, avail_cutoff)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Managing calendar dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mcal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"united-states_portland_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_calendar.csv.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mcal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'listing_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'available'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# This is needed because newer calendar files add cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mcal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcal_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcal_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcal_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Deal with different scrape times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m     \"\"\"\n\u001b[1;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This section creates a list of calendar and listing frames for each date range.\n",
    "my_cals =[]\n",
    "my_lists=[]\n",
    "for i in range (0,len(init_dates_2)-1):\n",
    "    if (adjustment_layer[i]!=0):\n",
    "        cal, listing = complex_extract_cal_list(str(arr_dates[i]), str(arr_dates[i] + adjustment_layer[i]), str(arr_dates[i] + 365 - (adjustment_layer[i]-1)))    \n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)\n",
    "    else:\n",
    "        cal, listing = complex_extract_cal_list(str(arr_dates[i]), str(arr_dates[i] + adjustment_layer[i]), str(arr_dates[i] + \\\n",
    "        (364)))\n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)\n",
    "                                                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilization of a more Pythonic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df[(listings_df['scrape_batch'] >= str(arr_dates[0])) & (listings_df['scrape_batch'] <= str(arr_dates[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ids that have been kept after listings data cleaning process (listings_df)\n",
    "\n",
    "ids = []\n",
    "unq_dates = []\n",
    "\n",
    "for cal in my_cals:\n",
    "    ids.append(cal['id'].unique())\n",
    "    unq_dates.append(cal['date'].unique())\n",
    "\n",
    "all_ids = np.sort(np.unique(np.concatenate(ids)))    \n",
    "all_dates = np.sort(np.unique(np.concatenate(unq_dates)))\n",
    "\n",
    "listings_df = listings_df[listings_df['drop_indicator'] == 0]\n",
    "\n",
    "late_appearance_df = listings_df.loc[((listings_df['first_appearance']== 1) & (listings_df['month'] >= 51))]\n",
    "    \n",
    "late_appearance_list = late_appearance_df['id'].tolist()\n",
    "\n",
    "all_ids = [x for x in all_ids if x not in late_appearance_list]\n",
    "\n",
    "all_ids = np.array(all_ids)\n",
    "# See if the data cleaning dropped any of the selected listings:\n",
    "mask = np.isin(all_ids, listings_df['id'].unique())\n",
    "all_ids = all_ids[mask] # Makes sure the ID is in the listings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary chunked processing and CSV creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(init=0,stop=100):\n",
    "    \"\"\"\n",
    "    This function produces and saves a final booked_df for ids within the range\n",
    "    init and stop in the all_ids array.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    unique_ids = all_ids[init:stop]# Final run should not have a restriction\n",
    "    unique_dates = all_dates # Here I set the limit on my dates. (Original 0 :688)\n",
    "\n",
    "    # Save the number of ids and dates\n",
    "\n",
    "    N_IDS = len(unique_ids) # I am using all the listing ids that I can right now.\n",
    "    N_DATES = len(unique_dates) # I am using only dates up to the end of FEB 2020\n",
    "    key_list = []\n",
    "\n",
    "    for an_id in unique_ids:\n",
    "        for a_date in unique_dates:\n",
    "            key_list.append(an_id.astype(str) + \":\" + a_date)\n",
    "    key_df = pd.DataFrame(key_list, columns=['key'])\n",
    "    def cal_file(df):\n",
    "        return df[(df['date'].isin(unique_dates)) & (df['id'].isin(unique_ids))].sort_values(by=['id','date']).reset_index(drop = True)\n",
    "    calendar_scrapes = []\n",
    "\n",
    "    for my_cal in my_cals:\n",
    "        calendar_scrapes.append(cal_file(my_cal))\n",
    "\n",
    "    N_CALS = len(calendar_scrapes)\n",
    "    i = 1\n",
    "    for cal_files in calendar_scrapes:\n",
    "        cal_files.loc[:, 'key'] = cal_files['id'].astype(str).str.strip() + \":\" + cal_files['date'].str.strip()\n",
    "        cal_files.columns = ['id' + str(i), 'date' + str(i), 'avail' + str(i), 'night_price' + str(i),\n",
    "                        'key']   \n",
    "        i+=1\n",
    "    cal_dfs = [key_df]\n",
    "\n",
    "    for i in range(N_CALS):\n",
    "        count = i + 1 \n",
    "        cal_dfs.append(calendar_scrapes[i][[('avail' + str(count)), 'key', ('night_price' + str(count))]])\n",
    "    \n",
    "\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['key'],\n",
    "                                            how='outer'), cal_dfs).fillna(np.nan) \n",
    "\n",
    "    df_merged.loc[:, \"id\"], df_merged.loc[:, \"date\"] = (df_merged['key'].str.split(\":\", expand=True)[0].astype(int),\n",
    "                                              df_merged['key'].str.split(\":\", expand=True)[1])\n",
    "    df_merged = df_merged.drop(columns=['key'])\n",
    "\n",
    "    df_merged = df_merged.drop_duplicates()\n",
    "    avail_cols = df_merged.columns[df_merged.columns.str.contains('avail')]\n",
    "    avail_cols = np.append(avail_cols, ('id','date'))\n",
    "\n",
    "    avail_arr = df_merged[avail_cols].values\n",
    "\n",
    "    avail_matrix = np.reshape(avail_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F') \n",
    "\n",
    "    avail_matrix = np.where(avail_matrix == 't', 1., avail_matrix) \n",
    "    avail_matrix = np.where(avail_matrix == 'f', 0., avail_matrix)\n",
    "    avail_change = np.diff(avail_matrix, axis=2)\n",
    "    case_1 = (np.nansum(avail_change, axis=2) == -1)*1 \n",
    "    def last_nonzero(arr, axis, invalid_val=-1):\n",
    "        \"\"\"\n",
    "        This function finds the last non-zero value of an array.\n",
    "        \"\"\"\n",
    "        mask = arr!= \"nan\"\n",
    "        val = arr.shape[axis] - np.flip(mask,axis=axis).argmax(axis=axis) - 1\n",
    "        return np.where(mask.any(axis=axis), val, invalid_val)\n",
    "    str_avail = avail_matrix.astype(str)\n",
    "    inds = last_nonzero(str_avail, axis=2, invalid_val=np.nan)\n",
    "\n",
    "    test_inds = inds\n",
    "    test_inds = np.reshape(test_inds, (N_DATES*N_IDS), order='F')\n",
    "    np.reshape(inds[0:10, 50:55],(10*5), order='F')\n",
    "    case2_df = pd.DataFrame(test_inds)\n",
    "    case2_df.columns = ['cal_index']\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].astype('Int64')\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].fillna(0) \n",
    "    my_vals = case2_df['cal_index'].values \n",
    "\n",
    "    reshape_avail = np.reshape(avail_matrix, (N_DATES*N_IDS*N_CALS,), order='F')\n",
    "\n",
    "    case2_df['id_index'] = (case2_df.index/N_DATES).astype(int)\n",
    "    case2_df['date_index'] = case2_df.index - N_DATES*case2_df['id_index']\n",
    "\n",
    "    indices = (case2_df['cal_index']*N_DATES*N_IDS + case2_df['id_index']*N_DATES + case2_df['date_index']).values\n",
    "\n",
    "    case2_result = reshape_avail[indices.astype(int)]\n",
    "    case2_result = np.reshape(case2_result, (N_DATES, N_IDS), order='F').astype(float)\n",
    "    cond_21 = (np.nansum(avail_change, axis=2) == 0)*1\n",
    "    cond_22 = (case2_result == 0)*1\n",
    "    cond_23 = (np.nanmax(avail_matrix,axis=2) == 1)*1\n",
    "    case_2 = cond_21*cond_22*cond_23\n",
    "    booked_mat = np.maximum(case_1, case_2)\n",
    "    booked_mat = np.reshape(booked_mat, (N_DATES*N_IDS,1), order='F')\n",
    "\n",
    "    booked_df = pd.DataFrame(booked_mat >= 1 , columns=['booked'])\n",
    "    booked_df.loc[:, \"id\"] = df_merged['id'].values\n",
    "    booked_df.loc[:, \"res_date\"] = df_merged['date'].values\n",
    "\n",
    "    never_avail = np.nanmax(avail_matrix,axis=2) != 1.\n",
    "    booked_df.loc[:, 'never_avail'] = np.reshape(never_avail, (N_DATES*N_IDS, 1), order='F')\n",
    "    price_cols = df_merged.columns[df_merged.columns.str.contains('night_price')]\n",
    "    price_cols = np.append(price_cols, ('id','date'))\n",
    "\n",
    "    price_arr = df_merged[price_cols].values\n",
    "\n",
    "    price_matrix = np.reshape(price_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F')\n",
    "    last_avail_flag = (np.roll(avail_change,0, axis=2) == -1)*1\n",
    "    last_avail_flag = np.where(last_avail_flag == 0, np.nan, last_avail_flag)\n",
    "\n",
    "    booked_price = np.nanmax(price_matrix[:,:,0:(N_CALS - 1)]*last_avail_flag,axis=2)\n",
    "    booked_df.loc[:, 'price'] = np.reshape(booked_price, (N_DATES*N_IDS,1), order='F')\n",
    "    last_date_avail = np.isfinite(last_avail_flag).argmax(2)\n",
    "    index_for_last = np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')\n",
    "    last_date = np.array(init_dates)[np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')]\n",
    "\n",
    "    # This ensures that the last date is only reported whenever the listing is identified as being booked.\n",
    "    booked_df.loc[:, \"last_date\"] = last_date\n",
    "    mask = booked_df.booked == False\n",
    "    column_name = 'last_date'\n",
    "    booked_df.loc[mask, column_name] = np.nan\n",
    "    price_matrix_np = np.float64(price_matrix)\n",
    "    def highest_index(a):\n",
    "        try:\n",
    "            return a[~np.isnan(a)][-1]\n",
    "        except: \n",
    "            return 99999\n",
    "    last_observed_price = np.apply_along_axis(highest_index, 2, price_matrix_np)\n",
    "    last_observed_price = np.where(last_observed_price == 99999, np.nan, last_observed_price)\n",
    "    last_observed_price = np.reshape(last_observed_price, (N_IDS*N_DATES), order='F')\n",
    "    booked_df = booked_df.sort_values(by=['id', 'res_date'])\n",
    "    booked_df.loc[:, \"all_prices\"] =  last_observed_price\n",
    "    booked_ind = (booked_df['booked']*1).values\n",
    "\n",
    "    final_prices = ((booked_ind)*booked_df['price'].fillna(0) + (1 - booked_ind)*booked_df['all_prices'].fillna(0)).values\n",
    "    booked_df.loc[:, 'final_prices'] = final_prices\n",
    "    booked_df['final_prices'].replace({0.:np.nan}, inplace=True)\n",
    "    booked_df.loc[:, \"seen_avail\"] = (booked_df['never_avail'] == False).values*1\n",
    "    booked_df = booked_df.sort_values(by='res_date').reset_index(drop=True)    \n",
    "    booked_df.loc[:, 'week_yr'] =  booked_df['res_date'].astype('datetime64').dt.strftime('%Y-%U')\n",
    "    booked_df.loc[:, 'mo_yr'] = booked_df['res_date'].astype('datetime64').dt.to_period('M')\n",
    "    booked_df_save = booked_df.copy()\n",
    "    booked_df_save.loc[:, \"mo_yr\"] = booked_df_save[\"mo_yr\"].astype(str)\n",
    "    \n",
    "    #Saving\n",
    "    os.chdir(data_path)\n",
    "    save_name = 'booked_df_v2' + \"_\" + str(j) + '.csv'\n",
    "    \n",
    "    booked_df_save.to_csv(save_name, index=False)\n",
    "    \n",
    "    #Comment out the above line and uncomment the line below to save chunks \n",
    "    #as compressed gzip files.This comes at the cost of process speed but saves disk space. \n",
    "    \n",
    "    #booked_df_save.to_csv(save_name, compression = 'gzip', index=False)\n",
    "    \n",
    "    print('chunk '+ str(j) + \" completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing function calls and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Iteration loop to prevent kernel crash\n",
    "j=0\n",
    "for i in range(0,82):\n",
    "    process(init=i*100, stop=(i*100)+100)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Concatenation and Production of Final Booked Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>booked</th>\n",
       "      <th>id</th>\n",
       "      <th>res_date</th>\n",
       "      <th>never_avail</th>\n",
       "      <th>price</th>\n",
       "      <th>last_date</th>\n",
       "      <th>all_prices</th>\n",
       "      <th>final_prices</th>\n",
       "      <th>seen_avail</th>\n",
       "      <th>week_yr</th>\n",
       "      <th>mo_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>880611</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>916916</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1024996</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>921586</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>1042814</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-35</td>\n",
       "      <td>2015-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  booked       id    res_date  never_avail  price last_date  \\\n",
       "0           0   False   880611  2015-09-02         True    NaN       NaN   \n",
       "1           1   False   916916  2015-09-02        False    NaN       NaN   \n",
       "2           2   False  1024996  2015-09-02         True    NaN       NaN   \n",
       "3           3   False   921586  2015-09-02         True    NaN       NaN   \n",
       "4           4   False  1042814  2015-09-02        False    NaN       NaN   \n",
       "\n",
       "   all_prices  final_prices  seen_avail  week_yr    mo_yr  \n",
       "0         NaN           NaN           0  2015-35  2015-09  \n",
       "1       150.0         150.0           1  2015-35  2015-09  \n",
       "2         NaN           NaN           0  2015-35  2015-09  \n",
       "3         NaN           NaN           0  2015-35  2015-09  \n",
       "4       160.0         160.0           1  2015-35  2015-09  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(data_path)\n",
    "\n",
    "# Read in one example chunk and look at it.\n",
    "test_df = pd.read_csv('booked_df_v2_3.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_simplified(file_name, keep_cols):\n",
    "    \n",
    "    \"\"\" This function takes a csv that has been saved through Pandas\n",
    "    and returns a numpy array. The keep_cols list specifies which columns\n",
    "    should be kept from the CSV file. \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[keep_cols]\n",
    "    \n",
    "    # Make booked a boolean\n",
    "    df.loc[:, 'booked'] = df['booked'].values*1\n",
    "    \n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booked_df_v2_0.csv\n",
      "booked_df_v2_1.csv\n",
      "booked_df_v2_2.csv\n",
      "booked_df_v2_3.csv\n",
      "booked_df_v2_4.csv\n",
      "booked_df_v2_5.csv\n",
      "booked_df_v2_6.csv\n",
      "booked_df_v2_7.csv\n",
      "booked_df_v2_8.csv\n",
      "booked_df_v2_9.csv\n",
      "booked_df_v2_10.csv\n",
      "booked_df_v2_11.csv\n",
      "booked_df_v2_12.csv\n",
      "booked_df_v2_13.csv\n",
      "booked_df_v2_14.csv\n",
      "booked_df_v2_15.csv\n",
      "booked_df_v2_16.csv\n",
      "booked_df_v2_17.csv\n",
      "booked_df_v2_18.csv\n",
      "booked_df_v2_19.csv\n",
      "booked_df_v2_20.csv\n",
      "booked_df_v2_21.csv\n",
      "booked_df_v2_22.csv\n",
      "booked_df_v2_23.csv\n",
      "booked_df_v2_24.csv\n",
      "booked_df_v2_25.csv\n",
      "booked_df_v2_26.csv\n",
      "booked_df_v2_27.csv\n",
      "booked_df_v2_28.csv\n",
      "booked_df_v2_29.csv\n",
      "booked_df_v2_30.csv\n",
      "booked_df_v2_31.csv\n",
      "booked_df_v2_32.csv\n",
      "booked_df_v2_33.csv\n",
      "booked_df_v2_34.csv\n",
      "booked_df_v2_35.csv\n",
      "booked_df_v2_36.csv\n",
      "booked_df_v2_37.csv\n",
      "booked_df_v2_38.csv\n",
      "booked_df_v2_39.csv\n",
      "booked_df_v2_40.csv\n",
      "booked_df_v2_41.csv\n",
      "booked_df_v2_42.csv\n",
      "booked_df_v2_43.csv\n",
      "booked_df_v2_44.csv\n",
      "booked_df_v2_45.csv\n",
      "booked_df_v2_46.csv\n",
      "booked_df_v2_47.csv\n",
      "booked_df_v2_48.csv\n",
      "booked_df_v2_49.csv\n",
      "booked_df_v2_50.csv\n",
      "booked_df_v2_51.csv\n",
      "booked_df_v2_52.csv\n",
      "booked_df_v2_53.csv\n",
      "booked_df_v2_54.csv\n",
      "booked_df_v2_55.csv\n",
      "booked_df_v2_56.csv\n",
      "booked_df_v2_57.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113593/3653569010.py:22: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  arr_i = chunk_simplified(file_i, my_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booked_df_v2_58.csv\n",
      "booked_df_v2_59.csv\n",
      "booked_df_v2_60.csv\n",
      "booked_df_v2_61.csv\n",
      "booked_df_v2_62.csv\n",
      "booked_df_v2_63.csv\n",
      "booked_df_v2_64.csv\n",
      "booked_df_v2_65.csv\n",
      "booked_df_v2_66.csv\n",
      "booked_df_v2_67.csv\n",
      "booked_df_v2_68.csv\n",
      "booked_df_v2_69.csv\n",
      "booked_df_v2_70.csv\n",
      "booked_df_v2_71.csv\n",
      "booked_df_v2_72.csv\n",
      "booked_df_v2_73.csv\n",
      "booked_df_v2_74.csv\n",
      "booked_df_v2_75.csv\n",
      "booked_df_v2_76.csv\n",
      "booked_df_v2_77.csv\n",
      "booked_df_v2_78.csv\n",
      "booked_df_v2_79.csv\n",
      "booked_df_v2_80.csv\n",
      "Minutes for concatenation loop:\n",
      "0.4992\n"
     ]
    }
   ],
   "source": [
    "# Concatenate chunks --\n",
    "\n",
    "# Extract all files in the directory\n",
    "dir_files = os.listdir(os.getcwd())\n",
    "\n",
    "# Count how many chunk files we have\n",
    "N_chunk_files = len([s for s in dir_files if \"booked_df_v2\" in s])-1\n",
    "\n",
    "# Select columns to keep \n",
    "# Delete 'Unnamed: 0', 'never_avail', 'price', 'all_prices' columns. \n",
    "my_cols = ['id', 'res_date','booked', 'seen_avail', 'final_prices', 'last_date', 'week_yr', 'mo_yr']\n",
    "\n",
    "file_name_base = 'booked_df_v2_'\n",
    "\n",
    "my_loop_timer = time.time()\n",
    "\n",
    "for ind in range(N_chunk_files):\n",
    "\n",
    "    file_i = file_name_base + str(ind) + \".csv\"\n",
    "    print(file_i)\n",
    "    \n",
    "    arr_i = chunk_simplified(file_i, my_cols)\n",
    "    \n",
    "    if ind == 0:\n",
    "        \n",
    "        full_arr = arr_i\n",
    "        \n",
    "    if ind > 0:\n",
    "        full_arr = np.concatenate([full_arr, arr_i], axis=0)\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "mins_to_run = (time.time() - my_loop_timer)/60\n",
    "\n",
    "print(\"Minutes for concatenation loop:\")\n",
    "print(np.round(mins_to_run, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compressed file --\n",
    "\n",
    "npz_file_name = \"Portland_booked_df\"\n",
    "\n",
    "my_date = str(datetime.now().year) + \"_\" + str(datetime.now().month) + \"_\" + str(datetime.now().day)\n",
    "\n",
    "my_save_timer = time.time()\n",
    "\n",
    "np.savez_compressed(npz_file_name, \n",
    "                    list_id = full_arr[:,0], \n",
    "                    res_date = full_arr[:,1],\n",
    "                    book_status = full_arr[:,2],\n",
    "                    seen_avail = full_arr[:, 3],\n",
    "                    prices = full_arr[:, 4],\n",
    "                    other_dates = full_arr[:, -3:], # These dates might end up being pretty important for merging later on.\n",
    "                    col_names = my_cols,\n",
    "                    date_of_file = my_date)\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "#save_mins_to_run = (time.time() - my_save_timer)/60\n",
    "\n",
    "#print(\"Minutes for concatenation loop:\")\n",
    "#print(np.round(save_mins_to_run, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list_id', 'res_date', 'book_status', 'seen_avail', 'prices', 'other_dates', 'col_names', 'date_of_file']\n"
     ]
    }
   ],
   "source": [
    "booked_df = np.load('Portland_booked_df.npz', allow_pickle=True)\n",
    "print(booked_df.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ids = booked_df['list_id']\n",
    "my_dates = booked_df['res_date']\n",
    "my_book_status = booked_df['book_status']\n",
    "my_seen_avail = booked_df['seen_avail']\n",
    "my_prices = booked_df['prices']\n",
    "my_other_dates = booked_df['other_dates']\n",
    "my_col_names = booked_df['col_names']\n",
    "my_date_of_file = booked_df['date_of_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12911400, 1),\n",
       " (12911400, 1),\n",
       " (12911400, 1),\n",
       " (12911400, 1),\n",
       " (12911400, 1),\n",
       " (3, 12911400, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([my_ids]).T.shape, np.array([my_dates]).T.shape, np.array([my_book_status]).T.shape, np.array([my_seen_avail]).T.shape, np.array([my_prices]).T.shape, np.array([my_other_dates]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejoined_1 = np.concatenate([np.array([my_ids]).T, np.array([my_dates]).T], axis=1)\n",
    "del my_ids \n",
    "del my_dates \n",
    "\n",
    "rejoined_2 = np.concatenate([rejoined_1, np.array([my_book_status]).T] , axis=1)\n",
    "del rejoined_1\n",
    "del my_book_status \n",
    "\n",
    "rejoined_3 = np.concatenate([rejoined_2, np.array([my_seen_avail]).T] , axis=1)\n",
    "del rejoined_2\n",
    "del my_seen_avail \n",
    "\n",
    "rejoined_final = np.concatenate([rejoined_3, np.array([my_prices]).T] , axis=1)\n",
    "del rejoined_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df = pd.DataFrame(rejoined_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'date', 'book_status', 'seen_avail', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df = booked_df[booked_df['price'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "booked_df.to_csv('rejoined_booked_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
