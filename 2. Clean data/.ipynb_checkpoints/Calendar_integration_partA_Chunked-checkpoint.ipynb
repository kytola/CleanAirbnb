{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar_integration_partA\n",
    "\n",
    "## Created June 19, 2020\n",
    "\n",
    "This file is the first half of the file \"CALENDAR DATA_05\", and it produces chunked dataframes named \"booked_df_\"x\".csv\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Set paths\n",
    "graphics_folder = cwd2 + '/3. Graphics/'\n",
    "data_path = cwd2 + '/Saved data/'\n",
    "csv_path = cwd2 + '/0. Raw data/'\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-09-14', '2016-09-04', '2017-06-05', '2017-03-05', '2018-01-16', '2017-11-13', '2018-04-11', '2016-04-05', '2017-09-12', '2017-10-04', '2016-11-06', '2018-12-10', '2017-07-06', '2018-05-13', '2016-08-04', '2018-02-08', '2017-01-04', '2017-08-06', '2016-01-01', '2018-11-07', '2019-01-13', '2015-12-02', '2016-12-08', '2015-11-02', '2017-05-07', '2019-02-06', '2016-05-03', '2017-12-09', '2016-02-03', '2018-10-09', '2018-08-14', '2018-07-10', '2016-06-03', '2017-04-07', '2016-07-04', '2017-02-09', '2015-09-02']\n"
     ]
    }
   ],
   "source": [
    "# This loop aggregates all calendar file names\n",
    "calFiles = []\n",
    "fileNames = os.listdir(csv_path)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"calendar.csv.gz\"):\n",
    "        calFiles.append(fileNames)\n",
    "        \n",
    "file_dates = []\n",
    "\n",
    "for i in range(len(calFiles)):\n",
    "    file_dates.append(calFiles[i].split('_')[2])\n",
    "    \n",
    "init_dates = file_dates\n",
    "del init_dates[9]\n",
    "del init_dates[16]\n",
    "print(init_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read 2nd stage cleaned dataframe\n",
    "os.chdir(data_path)\n",
    "\n",
    "dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "listings_df = pd.read_csv('2ndStageClean_Portland.csv.gz', compression = 'gzip', low_memory=False, parse_dates=dateCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def complex_extract_cal_list(date, cal_start, cal_end, update_cutoff=90, avail_cutoff=0):\n",
    "    warnings.simplefilter(\"ignore\", category=SettingWithCopyWarning)\n",
    "    \"\"\"\n",
    "    The point of this function is to remove listings that seem dormant \n",
    "    on Airbnb from the calendar files that we will be using.\n",
    "    \"\"\"\n",
    "    # Managing listings dataframe \n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv\", low_memory = False, parse_dates = dateCols)\n",
    "    \n",
    "    # Here I try to identify which listing calendars should be trusted, I try not to be too harsh.\n",
    "    small = list_file['calendar_updated'].str.split(' ', 3, expand=True)\n",
    "    small.columns = ['count', 'measure', 'length']\n",
    "    small = small[['count', 'measure']]\n",
    "    small = small.replace([\"days\", 'week', 'weeks', 'months', 'today', 'never','yesterday'], \n",
    "                          [1, 7, 7, 30, 0, 999, 1])\n",
    "    small['count'] = small['count'].replace('a', 1)\n",
    "    small = small.fillna(1)\n",
    "\n",
    "    list_file.loc[:, 'update_numeric'] = small['count'].astype(float)*small['measure'].astype(float)\n",
    "\n",
    "    checks = list_file[['update_numeric', 'instant_bookable', 'last_scraped', 'last_review', 'availability_365']]\n",
    "    checks.loc[:, 'DSR'] = (checks['last_scraped'] - checks['last_review']).dt.days\n",
    "    checks = checks[['update_numeric', 'instant_bookable', 'DSR', 'availability_365']]\n",
    "    \n",
    "\n",
    "    list_file.loc[:, 'DSR'] = checks['DSR']\n",
    "    list_file.loc[:, 'cal_trust'] = (((checks['update_numeric'] <= update_cutoff) | (checks['instant_bookable'] =='t') \n",
    "                                     | (checks['availability_365'] > avail_cutoff))*1) #      \n",
    "    trusted_ids = list_file[list_file['cal_trust'] == 1]['id'].unique()\n",
    "    \n",
    "    list_file['price'] = list_file.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "     # Reduce the size of variables in the listings dataframe.\n",
    "    list_file_small = list_file[['id', 'price', 'host_listings_count','bedrooms', 'bathrooms', 'neighbourhood', 'zipcode', 'room_type', \n",
    "                                 'instant_bookable', 'update_numeric', 'DSR', 'cal_trust', 'availability_365']]\n",
    "   \n",
    "    list_file_small.columns = ['id', 'headline_price', 'host_lists', 'bedrooms', \n",
    "                               'bathrooms', 'neigh', 'zip', 'type', 'instant', \n",
    "                               'DSupdate', 'DSReview', 'Active_flag', 'avail365']\n",
    "\n",
    "    # Managing calendar dataframe\n",
    "    cal_file = pd.read_csv(\"united-states_portland_\" + date + \"_calendar.csv.gz\", compression = 'gzip')\n",
    "    cal_file = cal_file[['listing_id', 'date','available', 'price']] # This is needed because newer calendar files add cols\n",
    "    cal_file = cal_file[(cal_file['date'] >= cal_start)& (cal_file['date'] <= cal_end)] # Deal with different scrape times\n",
    "    cal_file['price'] = cal_file.price.replace('[\\$,]', '', regex=True).astype(float)   # Prices converted to floats\n",
    "    \n",
    "    print(\"----Cleaning calendar and listings data for \" + date + \"----\")\n",
    "    print(\"Listing-dates removed: \" + str(float(len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))))\n",
    "    print(\"Unique listings removed: \" + str(float((len(cal_file) - len(cal_file[cal_file['listing_id'].isin(trusted_ids)]))/365)))\n",
    "    print(\"                                                           \")\n",
    "    \n",
    "    cal_file = cal_file[cal_file['listing_id'].isin(trusted_ids)] # Removes questionable calendar data\n",
    "   \n",
    "    # Rename columns\n",
    "    cal_file.columns = ['id', 'date', 'avail', 'night_price']\n",
    "    \n",
    "    return cal_file, list_file_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listings_df.loc[:, 'List_month'] = listings_df['List_month'] - listings_df['Occasional_LTR'] \n",
    "\n",
    "#listings_df = listings_df[listings_df['Occasional_LTR'] == 0] # This may mess up some of the dimensionsl\n",
    "\n",
    "os.chdir(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data cleaning and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-09-02' '2015-11-02' '2015-12-02' '2016-01-01' '2016-02-03'\n",
      " '2016-04-05' '2016-05-03' '2016-06-03' '2016-07-04' '2016-08-04'\n",
      " '2016-09-04' '2016-11-06' '2016-12-08' '2017-01-04' '2017-02-09'\n",
      " '2017-03-05' '2017-04-07' '2017-05-07' '2017-06-05' '2017-07-06'\n",
      " '2017-08-06' '2017-09-12' '2017-10-04' '2017-11-13' '2017-12-09'\n",
      " '2018-01-16' '2018-02-08' '2018-04-11' '2018-05-13' '2018-07-10'\n",
      " '2018-08-14' '2018-09-14' '2018-10-09' '2018-11-07' '2018-12-10'\n",
      " '2019-01-13' '2019-02-06']\n"
     ]
    }
   ],
   "source": [
    "#This section focuses on date formatting and array creation:\n",
    "arr_dates_init = np.array(init_dates).astype('datetime64')\n",
    "arr_dates_init.sort()\n",
    "arr_dates = arr_dates_init\n",
    "print(arr_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dates_2 = list((arr_dates).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "No issue\n",
      "1\n",
      "No issue\n",
      "2\n",
      "No issue\n",
      "3\n",
      "No issue\n",
      "4\n",
      "No issue\n",
      "5\n",
      "No issue\n",
      "6\n",
      "No issue\n",
      "7\n",
      "No issue\n",
      "8\n",
      "No issue\n",
      "9\n",
      "No issue\n",
      "10\n",
      "No issue\n",
      "11\n",
      "No issue\n",
      "12\n",
      "No issue\n",
      "13\n",
      "No issue\n",
      "14\n",
      "No issue\n",
      "15\n",
      "Issue!\n",
      "['2017-03-06T00:00:00.000000000' '2017-03-05T00:00:00.000000000']\n",
      "16\n",
      "Issue!\n",
      "['2017-04-07T00:00:00.000000000' '2017-04-08T00:00:00.000000000']\n",
      "17\n",
      "No issue\n",
      "18\n",
      "No issue\n",
      "19\n",
      "No issue\n",
      "20\n",
      "No issue\n",
      "21\n",
      "No issue\n",
      "22\n",
      "No issue\n",
      "23\n",
      "No issue\n",
      "24\n",
      "No issue\n",
      "25\n",
      "Issue!\n",
      "['2018-01-16T00:00:00.000000000' '2018-01-17T00:00:00.000000000']\n",
      "26\n",
      "No issue\n",
      "27\n",
      "Issue!\n",
      "['2018-04-11T00:00:00.000000000' '2018-04-12T00:00:00.000000000']\n",
      "28\n",
      "Issue!\n",
      "['2018-05-14T00:00:00.000000000' '2018-05-13T00:00:00.000000000']\n",
      "29\n",
      "No issue\n",
      "30\n",
      "No issue\n",
      "31\n",
      "No issue\n",
      "32\n",
      "Issue!\n",
      "['2018-10-09T00:00:00.000000000' '2018-10-12T00:00:00.000000000']\n",
      "33\n",
      "Issue!\n",
      "['2018-11-07T00:00:00.000000000' '2018-11-09T00:00:00.000000000']\n",
      "34\n",
      "No issue\n",
      "35\n",
      "No issue\n",
      "36\n",
      "No issue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loop simply identifies when a scrape took more than one day, \n",
    "# and creates an adjustment layer based on this information\n",
    "os.chdir(csv_path)\n",
    "i = 0\n",
    "adjustment_layer = []\n",
    "for date in init_dates_2:\n",
    "    dateCols =['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "    list_file = pd.read_csv(\"united-states_portland_\" + date + \"_listings.csv\", low_memory = False, parse_dates = dateCols)\n",
    "    print(i)\n",
    "    if len(list_file.last_scraped.unique()) > 1:\n",
    "        print(\"Issue!\")\n",
    "        print(list_file.last_scraped.unique())\n",
    "        adjustment_layer.append(len(list_file.last_scraped.unique()))\n",
    "    else:\n",
    "        print(\"No issue\")\n",
    "        adjustment_layer.append(0)\n",
    "    i += 1\n",
    "adjustment_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Cleaning calendar and listings data for 2015-09-02----\n",
      "Listing-dates removed: 730.0\n",
      "Unique listings removed: 2.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-11-02----\n",
      "Listing-dates removed: 1825.0\n",
      "Unique listings removed: 5.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2015-12-02----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-01-01----\n",
      "Listing-dates removed: 4015.0\n",
      "Unique listings removed: 11.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-02-03----\n",
      "Listing-dates removed: 25116.0\n",
      "Unique listings removed: 68.81095890410958\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-04-05----\n",
      "Listing-dates removed: 41610.0\n",
      "Unique listings removed: 114.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-05-03----\n",
      "Listing-dates removed: 45990.0\n",
      "Unique listings removed: 126.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-06-03----\n",
      "Listing-dates removed: 47815.0\n",
      "Unique listings removed: 131.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-07-04----\n",
      "Listing-dates removed: 49640.0\n",
      "Unique listings removed: 136.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-08-04----\n",
      "Listing-dates removed: 47815.0\n",
      "Unique listings removed: 131.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-09-04----\n",
      "Listing-dates removed: 49275.0\n",
      "Unique listings removed: 135.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-11-06----\n",
      "Listing-dates removed: 65700.0\n",
      "Unique listings removed: 180.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2016-12-08----\n",
      "Listing-dates removed: 70980.0\n",
      "Unique listings removed: 194.46575342465752\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-01-04----\n",
      "Listing-dates removed: 78840.0\n",
      "Unique listings removed: 216.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-02-09----\n",
      "Listing-dates removed: 81364.0\n",
      "Unique listings removed: 222.9150684931507\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-03-05----\n",
      "Listing-dates removed: 86031.0\n",
      "Unique listings removed: 235.7013698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-04-07----\n",
      "Listing-dates removed: 92202.0\n",
      "Unique listings removed: 252.60821917808218\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-05-07----\n",
      "Listing-dates removed: 92198.0\n",
      "Unique listings removed: 252.5972602739726\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-06-05----\n",
      "Listing-dates removed: 88481.0\n",
      "Unique listings removed: 242.413698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-07-06----\n",
      "Listing-dates removed: 86870.0\n",
      "Unique listings removed: 238.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-08-06----\n",
      "Listing-dates removed: 89180.0\n",
      "Unique listings removed: 244.32876712328766\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-09-12----\n",
      "Listing-dates removed: 93805.0\n",
      "Unique listings removed: 257.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-10-04----\n",
      "Listing-dates removed: 104390.0\n",
      "Unique listings removed: 286.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-11-13----\n",
      "Listing-dates removed: 113150.0\n",
      "Unique listings removed: 310.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2017-12-09----\n",
      "Listing-dates removed: 127400.0\n",
      "Unique listings removed: 349.041095890411\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-01-16----\n",
      "Listing-dates removed: 139029.0\n",
      "Unique listings removed: 380.9013698630137\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-02-08----\n",
      "Listing-dates removed: 139611.0\n",
      "Unique listings removed: 382.4958904109589\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-04-11----\n",
      "Listing-dates removed: 154638.0\n",
      "Unique listings removed: 423.6657534246575\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-05-13----\n",
      "Listing-dates removed: 151734.0\n",
      "Unique listings removed: 415.7095890410959\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-07-10----\n",
      "Listing-dates removed: 152935.0\n",
      "Unique listings removed: 419.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-08-14----\n",
      "Listing-dates removed: 155490.0\n",
      "Unique listings removed: 426.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-09-14----\n",
      "Listing-dates removed: 157680.0\n",
      "Unique listings removed: 432.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-10-09----\n",
      "Listing-dates removed: 158994.0\n",
      "Unique listings removed: 435.6\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-11-07----\n",
      "Listing-dates removed: 165165.0\n",
      "Unique listings removed: 452.5068493150685\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2018-12-10----\n",
      "Listing-dates removed: 166075.0\n",
      "Unique listings removed: 455.0\n",
      "                                                           \n",
      "----Cleaning calendar and listings data for 2019-01-13----\n",
      "Listing-dates removed: 174356.0\n",
      "Unique listings removed: 477.6876712328767\n",
      "                                                           \n"
     ]
    }
   ],
   "source": [
    "#This section creates a list of calendar and listing frames for each date range.\n",
    "my_cals =[]\n",
    "my_lists=[]\n",
    "for i in range (0,len(init_dates_2)-1):\n",
    "    if (adjustment_layer[i]!=0):\n",
    "        cal, listing = complex_extract_cal_list(str(arr_dates[i]), str(arr_dates[i] + adjustment_layer[i]), str(arr_dates[i] + 365 - (adjustment_layer[i]-1)))    \n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)\n",
    "    else:\n",
    "        cal, listing = complex_extract_cal_list(str(arr_dates[i]), str(arr_dates[i] + adjustment_layer[i]), str(arr_dates[i] + \\\n",
    "        (364)))\n",
    "        my_cals.append(cal)\n",
    "        my_lists.append(listing)\n",
    "                                                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilization of a more Pythonic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df[(listings_df['scrape_batch'] >= str(arr_dates[0])) & (listings_df['scrape_batch'] <= str(arr_dates[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ids that have been kept after listings data cleaning process (listings_df)\n",
    "\n",
    "ids = []\n",
    "unq_dates = []\n",
    "\n",
    "for cal in my_cals:\n",
    "    ids.append(cal['id'].unique())\n",
    "    unq_dates.append(cal['date'].unique())\n",
    "\n",
    "all_ids = np.sort(np.unique(np.concatenate(ids)))    \n",
    "all_dates = np.sort(np.unique(np.concatenate(unq_dates)))\n",
    "\n",
    "listings_df = listings_df[listings_df['drop_indicator'] == 0]\n",
    "\n",
    "late_appearance_df = listings_df.loc[((listings_df['first_appearance']== 1) & (listings_df['month'] >= 51))]\n",
    "    \n",
    "late_appearance_list = late_appearance_df['id'].tolist()\n",
    "\n",
    "all_ids = [x for x in all_ids if x not in late_appearance_list]\n",
    "\n",
    "all_ids = np.array(all_ids)\n",
    "# See if the data cleaning dropped any of the selected listings:\n",
    "mask = np.isin(all_ids, listings_df['id'].unique())\n",
    "all_ids = all_ids[mask] # Makes sure the ID is in the listings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary chunked processing and CSV creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(init=0,stop=100):\n",
    "    \"\"\"\n",
    "    This function produces and saves a final booked_df for ids within the range\n",
    "    init and stop in the all_ids array.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    unique_ids = all_ids[init:stop]# Final run should not have a restriction\n",
    "    unique_dates = all_dates # Here I set the limit on my dates. (Original 0 :688)\n",
    "\n",
    "    # Save the number of ids and dates\n",
    "\n",
    "    N_IDS = len(unique_ids) # I am using all the listing ids that I can right now.\n",
    "    N_DATES = len(unique_dates) # I am using only dates up to the end of FEB 2020\n",
    "    key_list = []\n",
    "\n",
    "    for an_id in unique_ids:\n",
    "        for a_date in unique_dates:\n",
    "            key_list.append(an_id.astype(str) + \":\" + a_date)\n",
    "    key_df = pd.DataFrame(key_list, columns=['key'])\n",
    "    def cal_file(df):\n",
    "        return df[(df['date'].isin(unique_dates)) & (df['id'].isin(unique_ids))].sort_values(by=['id','date']).reset_index(drop = True)\n",
    "    calendar_scrapes = []\n",
    "\n",
    "    for my_cal in my_cals:\n",
    "        calendar_scrapes.append(cal_file(my_cal))\n",
    "\n",
    "    N_CALS = len(calendar_scrapes)\n",
    "    i = 1\n",
    "    for cal_files in calendar_scrapes:\n",
    "        cal_files.loc[:, 'key'] = cal_files['id'].astype(str).str.strip() + \":\" + cal_files['date'].str.strip()\n",
    "        cal_files.columns = ['id' + str(i), 'date' + str(i), 'avail' + str(i), 'night_price' + str(i),\n",
    "                        'key']   \n",
    "        i+=1\n",
    "    cal_dfs = [key_df]\n",
    "\n",
    "    for i in range(N_CALS):\n",
    "        count = i + 1 \n",
    "        cal_dfs.append(calendar_scrapes[i][[('avail' + str(count)), 'key', ('night_price' + str(count))]])\n",
    "    \n",
    "\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['key'],\n",
    "                                            how='outer'), cal_dfs).fillna(np.nan) \n",
    "\n",
    "    df_merged.loc[:, \"id\"], df_merged.loc[:, \"date\"] = (df_merged['key'].str.split(\":\", expand=True)[0].astype(int),\n",
    "                                              df_merged['key'].str.split(\":\", expand=True)[1])\n",
    "    df_merged = df_merged.drop(columns=['key'])\n",
    "\n",
    "    df_merged = df_merged.drop_duplicates()\n",
    "    avail_cols = df_merged.columns[df_merged.columns.str.contains('avail')]\n",
    "    avail_cols = np.append(avail_cols, ('id','date'))\n",
    "\n",
    "    avail_arr = df_merged[avail_cols].values\n",
    "\n",
    "    avail_matrix = np.reshape(avail_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F') \n",
    "\n",
    "    avail_matrix = np.where(avail_matrix == 't', 1., avail_matrix) \n",
    "    avail_matrix = np.where(avail_matrix == 'f', 0., avail_matrix)\n",
    "    avail_change = np.diff(avail_matrix, axis=2)\n",
    "    case_1 = (np.nansum(avail_change, axis=2) == -1)*1 \n",
    "    def last_nonzero(arr, axis, invalid_val=-1):\n",
    "        \"\"\"\n",
    "        This function finds the last non-zero value of an array.\n",
    "        \"\"\"\n",
    "        mask = arr!= \"nan\"\n",
    "        val = arr.shape[axis] - np.flip(mask,axis=axis).argmax(axis=axis) - 1\n",
    "        return np.where(mask.any(axis=axis), val, invalid_val)\n",
    "    str_avail = avail_matrix.astype(str)\n",
    "    inds = last_nonzero(str_avail, axis=2, invalid_val=np.nan)\n",
    "\n",
    "    test_inds = inds\n",
    "    test_inds = np.reshape(test_inds, (N_DATES*N_IDS), order='F')\n",
    "    np.reshape(inds[0:10, 50:55],(10*5), order='F')\n",
    "    case2_df = pd.DataFrame(test_inds)\n",
    "    case2_df.columns = ['cal_index']\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].astype('Int64')\n",
    "    case2_df['cal_index'] = case2_df['cal_index'].fillna(0) \n",
    "    my_vals = case2_df['cal_index'].values \n",
    "\n",
    "    reshape_avail = np.reshape(avail_matrix, (N_DATES*N_IDS*N_CALS,), order='F')\n",
    "\n",
    "    case2_df['id_index'] = (case2_df.index/N_DATES).astype(int)\n",
    "    case2_df['date_index'] = case2_df.index - N_DATES*case2_df['id_index']\n",
    "\n",
    "    indices = (case2_df['cal_index']*N_DATES*N_IDS + case2_df['id_index']*N_DATES + case2_df['date_index']).values\n",
    "\n",
    "    case2_result = reshape_avail[indices.astype(int)]\n",
    "    case2_result = np.reshape(case2_result, (N_DATES, N_IDS), order='F').astype(float)\n",
    "    cond_21 = (np.nansum(avail_change, axis=2) == 0)*1\n",
    "    cond_22 = (case2_result == 0)*1\n",
    "    cond_23 = (np.nanmax(avail_matrix,axis=2) == 1)*1\n",
    "    case_2 = cond_21*cond_22*cond_23\n",
    "    booked_mat = np.maximum(case_1, case_2)\n",
    "    booked_mat = np.reshape(booked_mat, (N_DATES*N_IDS,1), order='F')\n",
    "\n",
    "    booked_df = pd.DataFrame(booked_mat >= 1 , columns=['booked'])\n",
    "    booked_df.loc[:, \"id\"] = df_merged['id'].values\n",
    "    booked_df.loc[:, \"res_date\"] = df_merged['date'].values\n",
    "\n",
    "    never_avail = np.nanmax(avail_matrix,axis=2) != 1.\n",
    "    booked_df.loc[:, 'never_avail'] = np.reshape(never_avail, (N_DATES*N_IDS, 1), order='F')\n",
    "    price_cols = df_merged.columns[df_merged.columns.str.contains('night_price')]\n",
    "    price_cols = np.append(price_cols, ('id','date'))\n",
    "\n",
    "    price_arr = df_merged[price_cols].values\n",
    "\n",
    "    price_matrix = np.reshape(price_arr[:,0:N_CALS], (int(N_DATES), N_IDS, N_CALS), order='F')\n",
    "    last_avail_flag = (np.roll(avail_change,0, axis=2) == -1)*1\n",
    "    last_avail_flag = np.where(last_avail_flag == 0, np.nan, last_avail_flag)\n",
    "\n",
    "    booked_price = np.nanmax(price_matrix[:,:,0:(N_CALS - 1)]*last_avail_flag,axis=2)\n",
    "    booked_df.loc[:, 'price'] = np.reshape(booked_price, (N_DATES*N_IDS,1), order='F')\n",
    "    last_date_avail = np.isfinite(last_avail_flag).argmax(2)\n",
    "    index_for_last = np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')\n",
    "    last_date = np.array(init_dates)[np.reshape(last_date_avail, (N_DATES*N_IDS,), order='F')]\n",
    "\n",
    "    # This ensures that the last date is only reported whenever the listing is identified as being booked.\n",
    "    booked_df.loc[:, \"last_date\"] = last_date\n",
    "    mask = booked_df.booked == False\n",
    "    column_name = 'last_date'\n",
    "    booked_df.loc[mask, column_name] = np.nan\n",
    "    price_matrix_np = np.float64(price_matrix)\n",
    "    def highest_index(a):\n",
    "        try:\n",
    "            return a[~np.isnan(a)][-1]\n",
    "        except: \n",
    "            return 99999\n",
    "    last_observed_price = np.apply_along_axis(highest_index, 2, price_matrix_np)\n",
    "    last_observed_price = np.where(last_observed_price == 99999, np.nan, last_observed_price)\n",
    "    last_observed_price = np.reshape(last_observed_price, (N_IDS*N_DATES), order='F')\n",
    "    booked_df = booked_df.sort_values(by=['id', 'res_date'])\n",
    "    booked_df.loc[:, \"all_prices\"] =  last_observed_price\n",
    "    booked_ind = (booked_df['booked']*1).values\n",
    "\n",
    "    final_prices = ((booked_ind)*booked_df['price'].fillna(0) + (1 - booked_ind)*booked_df['all_prices'].fillna(0)).values\n",
    "    booked_df.loc[:, 'final_prices'] = final_prices\n",
    "    booked_df['final_prices'].replace({0.:np.nan}, inplace=True)\n",
    "    booked_df.loc[:, \"seen_avail\"] = (booked_df['never_avail'] == False).values*1\n",
    "    booked_df = booked_df.sort_values(by='res_date').reset_index(drop=True)    \n",
    "    booked_df.loc[:, 'week_yr'] =  booked_df['res_date'].astype('datetime64').dt.strftime('%Y-%U')\n",
    "    booked_df.loc[:, 'mo_yr'] = booked_df['res_date'].astype('datetime64').dt.to_period('M')\n",
    "    booked_df_save = booked_df.copy()\n",
    "    booked_df_save.loc[:, \"mo_yr\"] = booked_df_save[\"mo_yr\"].astype(str)\n",
    "    \n",
    "    #Saving\n",
    "    os.chdir(data_path)\n",
    "    save_name = 'booked_df_v2' + \"_\" + str(j) + '.csv'\n",
    "    \n",
    "    booked_df_save.to_csv(save_name)\n",
    "    \n",
    "    #Comment out the above line and uncomment the line below to save chunks \n",
    "    #as compressed gzip files.This comes at the cost of process speed but saves disk space. \n",
    "    \n",
    "    #booked_df_save.to_csv(save_name, compression = 'gzip')\n",
    "    \n",
    "    print('chunk '+ str(j) + \" completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing function calls and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 completed\n",
      "chunk 1 completed\n",
      "chunk 2 completed\n",
      "chunk 3 completed\n",
      "chunk 4 completed\n",
      "chunk 5 completed\n",
      "chunk 6 completed\n",
      "chunk 7 completed\n",
      "chunk 8 completed\n",
      "chunk 9 completed\n",
      "chunk 10 completed\n",
      "chunk 11 completed\n",
      "chunk 12 completed\n",
      "chunk 13 completed\n",
      "chunk 14 completed\n",
      "chunk 15 completed\n",
      "chunk 16 completed\n",
      "chunk 17 completed\n",
      "chunk 18 completed\n",
      "chunk 19 completed\n",
      "chunk 20 completed\n",
      "chunk 21 completed\n",
      "chunk 22 completed\n",
      "chunk 23 completed\n",
      "chunk 24 completed\n",
      "chunk 25 completed\n",
      "chunk 26 completed\n",
      "chunk 27 completed\n",
      "chunk 28 completed\n",
      "chunk 29 completed\n",
      "chunk 30 completed\n",
      "chunk 31 completed\n",
      "chunk 32 completed\n",
      "chunk 33 completed\n",
      "chunk 34 completed\n",
      "chunk 35 completed\n",
      "chunk 36 completed\n",
      "chunk 37 completed\n",
      "chunk 38 completed\n",
      "chunk 39 completed\n",
      "chunk 40 completed\n",
      "chunk 41 completed\n",
      "chunk 42 completed\n",
      "chunk 43 completed\n",
      "chunk 44 completed\n",
      "chunk 45 completed\n",
      "chunk 46 completed\n",
      "chunk 47 completed\n",
      "chunk 48 completed\n",
      "chunk 49 completed\n",
      "chunk 50 completed\n",
      "chunk 51 completed\n",
      "chunk 52 completed\n",
      "chunk 53 completed\n",
      "chunk 54 completed\n",
      "chunk 55 completed\n",
      "chunk 56 completed\n",
      "chunk 57 completed\n",
      "chunk 58 completed\n",
      "chunk 59 completed\n",
      "chunk 60 completed\n",
      "chunk 61 completed\n",
      "chunk 62 completed\n",
      "chunk 63 completed\n",
      "chunk 64 completed\n",
      "chunk 65 completed\n",
      "chunk 66 completed\n",
      "chunk 67 completed\n",
      "chunk 68 completed\n",
      "chunk 69 completed\n",
      "chunk 70 completed\n",
      "chunk 71 completed\n",
      "chunk 72 completed\n",
      "chunk 73 completed\n",
      "chunk 74 completed\n",
      "chunk 75 completed\n",
      "chunk 76 completed\n",
      "chunk 77 completed\n",
      "chunk 78 completed\n",
      "chunk 79 completed\n",
      "chunk 80 completed\n",
      "chunk 81 completed\n"
     ]
    }
   ],
   "source": [
    "#Iteration loop to prevent kernel crash\n",
    "j=0\n",
    "for i in range(0,82):\n",
    "    process(init=i*100, stop=(i*100)+100)\n",
    "    j+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
